# =============================================================================
# JB CALLING - Docker Stack Configuration
# =============================================================================
# Version: 2.0
# Description: Docker Swarm stack file cho toàn bộ hệ thống
# Usage: docker stack deploy -c stack.yml translation
# =============================================================================

version: '3.8'

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  frontend:
    driver: overlay
    attachable: true
  backend:
    driver: overlay
    internal: true
  monitoring:
    driver: overlay

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  postgres_data:
  redis_data:
  models_cache:
  prometheus_data:
  grafana_data:
  loki_data:

# =============================================================================
# SECRETS (TODO: Sử dụng Docker secrets trong production)
# =============================================================================
secrets:
  postgres_password:
    external: true
  redis_password:
    external: true
  jwt_secret:
    external: true

# =============================================================================
# SERVICES
# =============================================================================
services:

  # ===========================================================================
  # DATABASE - PostgreSQL
  # ===========================================================================
  postgres:
    image: postgres:15-alpine
    networks:
      - backend
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    deploy:
      placement:
        constraints:
          - node.labels.role == manager
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ===========================================================================
  # CACHE/QUEUE - Redis
  # ===========================================================================
  redis:
    image: redis:7-alpine
    networks:
      - backend
    volumes:
      - redis_data:/data
    command: 
      - redis-server
      - --requirepass
      - ${REDIS_PASSWORD}
      - --maxmemory
      - ${REDIS_MAXMEMORY}
      - --maxmemory-policy
      - ${REDIS_MAXMEMORY_POLICY}
      - --appendonly
      - "yes"
    deploy:
      placement:
        constraints:
          - node.labels.role == manager
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ===========================================================================
  # API - FastAPI Backend
  # ===========================================================================
  api:
    image: jackboun11/jbcalling-api:1.0.0
    networks:
      - frontend
      - backend
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - JWT_ALGORITHM=${JWT_ALGORITHM}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES}
      - REFRESH_TOKEN_EXPIRE_DAYS=${REFRESH_TOKEN_EXPIRE_DAYS}
      - APP_ENV=${APP_ENV}
      - CORS_ORIGINS=${CORS_ORIGINS}
    depends_on:
      - postgres
      - redis
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.role == worker
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8000/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ===========================================================================
  # SIGNALING - WebSocket Signaling Server
  # ===========================================================================
  signaling:
    image: jackboun11/jbcalling-api:1.0.0
    networks:
      - frontend
      - backend
    command: ["uvicorn", "signaling:app", "--host", "0.0.0.0", "--port", "8001"]
    environment:
      - APP_ENV=${APP_ENV}
    depends_on:
      - redis
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.role == worker
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8001/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ===========================================================================
  # FRONTEND - React Application
  # ===========================================================================
  frontend:
    image: jackboun11/jbcalling-frontend:1.0.1
    networks:
      - frontend
    environment:
      - VITE_API_URL=http://api:8000
      - VITE_WS_URL=ws://signaling:8001
    depends_on:
      - api
      - signaling
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.role == worker
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    ports:
      - "${FRONTEND_PORT:-80}:80"

  # ===========================================================================
  # WEBRTC - MediaSoup Gateway (Phase 3)
  # ===========================================================================
  # TODO: Implement MediaSoup service
  # Chạy trên Instance 2 (translation02)
  
  # ===========================================================================
  # STT - Speech-to-Text Service (Phase 3.1) ✅
  # ===========================================================================
  stt:
    image: jackboun11/jbcalling-stt:phowhisper
    networks:
      - backend
      - monitoring
    volumes:
      - models_cache:/root/.cache  # Cache cho HuggingFace models
    environment:
      # Model configuration
      - USE_PHOWHISPER=true           # Enable PhoWhisper for Vietnamese
      - USE_FASTER_WHISPER=true       # Enable faster-whisper fallback
      - MODEL_SIZE=small              # faster-whisper model size
      - COMPUTE_TYPE=int8             # INT8 quantization for CPU
      - DEVICE=cpu                    # CPU-only (no GPU)
      - OMP_NUM_THREADS=4             # CPU threads for inference
      # Performance tuning
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # App settings
      - APP_ENV=${APP_ENV:-production}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      - redis
    deploy:
      replicas: 2                     # 2 replicas for high availability
      placement:
        constraints:
          - node.labels.instance == translation01  # CPU-optimized instance
        preferences:
          - spread: node.labels.instance
      resources:
        limits:
          cpus: '2.0'                 # Max 2 CPUs per container
          memory: 3G                  # 3GB RAM (PhoWhisper + faster-whisper)
        reservations:
          cpus: '1.0'                 # Reserve 1 CPU
          memory: 2G                  # Reserve 2GB RAM
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1                # Update one at a time
        delay: 30s                    # Wait 30s between updates
        order: start-first            # Start new before stopping old
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 10s
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s               # Long start period for model loading
    labels:
      # Traefik labels (nếu cần expose qua HTTPS)
      - "traefik.enable=false"        # Internal service only
      # Prometheus metrics
      - "prometheus.scrape=true"
      - "prometheus.port=8002"
      - "prometheus.path=/metrics"
  
  # ===========================================================================
  # TRANSLATION - NLLB Service (Phase 3)
  # ===========================================================================
  # TODO: Implement Translation service
  # Chạy trên Instance 1 (translation01)
  
  # ===========================================================================
  # TTS - Text-to-Speech Services (Phase 3)
  # ===========================================================================
  # TODO: Implement TTS services (gTTS, XTTS async, pyttsx3)
  
  # ===========================================================================
  # BACKGROUND JOBS - Celery Workers (Phase 3)
  # ===========================================================================
  # TODO: Implement Celery workers
  
  # ===========================================================================
  # MONITORING - Prometheus
  # ===========================================================================
  prometheus:
    image: prom/prometheus:latest
    networks:
      - monitoring
      - backend
    volumes:
      - prometheus_data:/prometheus
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME}'
    deploy:
      placement:
        constraints:
          - node.labels.role == monitoring
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
      restart_policy:
        condition: on-failure
    ports:
      - "${PROMETHEUS_PORT}:9090"

  # ===========================================================================
  # MONITORING - Grafana
  # ===========================================================================
  grafana:
    image: grafana/grafana:latest
    networks:
      - monitoring
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./configs/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=
    deploy:
      placement:
        constraints:
          - node.labels.role == monitoring
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
    ports:
      - "${GRAFANA_PORT}:3000"

  # ===========================================================================
  # LOGGING - Loki
  # ===========================================================================
  loki:
    image: grafana/loki:latest
    networks:
      - monitoring
    volumes:
      - loki_data:/loki
      - ./configs/loki.yml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    deploy:
      placement:
        constraints:
          - node.labels.role == monitoring
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
    ports:
      - "${LOKI_PORT}:3100"

  # ===========================================================================
  # LOGGING - Promtail
  # ===========================================================================
  promtail:
    image: grafana/promtail:latest
    networks:
      - monitoring
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./configs/promtail.yml:/etc/promtail/config.yml:ro
    command: -config.file=/etc/promtail/config.yml
    deploy:
      mode: global  # Chạy trên mọi node
      resources:
        limits:
          cpus: '0.2'
          memory: 256M
      restart_policy:
        condition: on-failure

  # ===========================================================================
  # LOAD BALANCER - Traefik (TODO)
  # ===========================================================================
  # TODO: Implement Traefik cho load balancing và auto SSL

# =============================================================================
# NOTES
# =============================================================================
# 
# Deployment:
#   docker stack deploy -c stack.yml translation
#
# Scale service:
#   docker service scale translation_api=4
#
# Update service:
#   docker service update --image new-image:tag translation_api
#
# View logs:
#   docker service logs -f translation_api
#
# Remove stack:
#   docker stack rm translation
#
# =============================================================================
