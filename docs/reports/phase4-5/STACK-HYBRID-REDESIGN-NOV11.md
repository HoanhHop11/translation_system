# Stack-Hybrid Architecture Redesign

**Date**: November 11, 2025  
**Status**: Design Phase  
**Phase**: 4-5 Gateway Fix - Cross-Node Routing Solution  

---

## üî¥ Problem Statement

### Current Issue
- **504 Gateway Timeout** khi access https://jbcalling.site
- Traefik (tr√™n translation01) KH√îNG TH·ªÇ route ƒë·∫øn frontend containers (tr√™n translation03)
- Root cause: Docker Swarm overlay network cross-node routing failure

### Evidence
```bash
# Frontend placement
docker service ps translation_frontend
# Result: ALL 3 replicas on translation03

# Traefik location  
docker service ps translation_traefik
# Result: On translation01 (manager node)

# Connection test
curl -I https://jbcalling.site/
# Result: 504 Gateway Timeout after 30s

# Traefik logs
docker service logs translation_traefik | grep 504
# "GET / HTTP/2.0" 504 - "frontend@docker" "http://10.0.5.27:80" 30002ms
```

### Deployed vs File Mismatch
```bash
# File: stack-hybrid.yml line 316-318
# Comment: "B·ªé placement constraint ƒë·ªÉ frontend c√≥ th·ªÉ ch·∫°y c√πng node v·ªõi Traefik"

# Reality: Deployed service
docker service inspect translation_frontend --format '{{.Spec.TaskTemplate.Placement}}'
# {
#   "Constraints": ["node.labels.instance == translation03"]  
# }
```

**K·∫øt lu·∫≠n**: Service ƒëang deploy v·∫´n c√≥ placement constraint m·∫∑c d√π file ƒë√£ comment v·ªÅ vi·ªác b·ªè n√≥.

---

## üîç Research Findings

### Docker Swarm + Traefik Cross-Node Issues

**Source 1: Traefik Community Forum**
> "Service discovery work perfectly fine, the only issue is that i cannot access my services if deployed on the worker node, if i deploy them on the manager node i can access them."
> 
> "From within the traefik container I can make a wget on a service running in the worker node therefore i assume that the swarm network is working well. Running my apps on the manager node, work well."

**Source 2: Reddit /r/Traefik**
> "Traefik picks it up and starts routing HTTP traffic to it ONLY when I force it to the same node with a constraint."

**Source 3: Docker Swarm Overlay Network Issue #43052**
> "overlay network exhaustion" - VIPs exhausted after ~100 services on same overlay network
> "no error is visible to a docker user. Only sys-admins can access the reason services are stuck"

### Key Insights

1. **Cross-node overlay routing** l√† v·∫•n ƒë·ªÅ ph·ªï bi·∫øn v·ªõi Traefik + Swarm
2. **Solution pattern**: Services c·∫ßn route b·ªüi Traefik n√™n:
   - Kh√¥ng c√≥ placement constraint (ƒë·ªÉ Swarm t·ª± ph√¢n ph·ªëi)
   - HO·∫∂C co-locate c√πng node v·ªõi Traefik
3. **Swarm ingress mesh** ho·∫°t ƒë·ªông t·ªët KHI kh√¥ng c√≥ placement constraints
4. **Overlay network VIP exhaustion** c√≥ th·ªÉ x·∫£y ra v·ªõi nhi·ªÅu services

---

## üéØ Proposed Solution

### Option A: Remove Placement Constraints (RECOMMENDED)

**Strategy**: ƒê·ªÉ Docker Swarm t·ª± ƒë·ªông ph√¢n ph·ªëi services, s·ª≠ d·ª•ng Swarm's ingress routing mesh

**Changes needed**:
1. **Frontend service**: B·ªè placement constraint ho√†n to√†n
2. **Signaling service**: B·ªè placement constraint (hi·ªán ƒëang force translation03)
3. **Keep MediaSoup placement**: V·∫´n gi·ªØ tr√™n translation02 (c·∫ßn UDP mode: host)

**Advantages**:
‚úÖ ƒê∆°n gi·∫£n, √≠t config  
‚úÖ T·∫≠n d·ª•ng Swarm routing mesh  
‚úÖ T·ª± ƒë·ªông load balancing  
‚úÖ Kh√¥ng overload manager node  
‚úÖ D·ªÖ scale trong t∆∞∆°ng lai  
‚úÖ Proven pattern t·ª´ community  

**Disadvantages**:
‚ö†Ô∏è Kh√¥ng ki·ªÉm so√°t ƒë∆∞·ª£c service ch·∫°y tr√™n node n√†o  
‚ö†Ô∏è C·∫ßn verify ingress mesh ho·∫°t ƒë·ªông ƒë√∫ng  

### Option B: Co-locate Frontend v·ªõi Traefik

**Strategy**: ƒê·∫∑t frontend replicas l√™n translation01 (c√πng node v·ªõi Traefik)

**Changes needed**:
```yaml
frontend:
  deploy:
    placement:
      constraints:
        - node.labels.instance == translation01
```

**Advantages**:
‚úÖ Guarantee routing ho·∫°t ƒë·ªông  
‚úÖ Kh√¥ng ph·ª• thu·ªôc ingress mesh  
‚úÖ Predictable placement  

**Disadvantages**:
‚ùå Overload manager node (30GB RAM nh∆∞ng ƒë√£ c√≥ nhi·ªÅu services)  
‚ùå Single point of failure cho frontend  
‚ùå Kh√¥ng t·∫≠n d·ª•ng worker nodes  
‚ùå Kh√≥ scale horizontally  

### Option C: Hybrid Approach

**Strategy**: 
- Frontend: Kh√¥ng c√≥ constraint, ph√¢n ph·ªëi t·ª± ƒë·ªông
- Signaling: Ph√¢n ph·ªëi t·ª± ƒë·ªông HO·∫∂C prefer translation02/03 (worker nodes)
- MediaSoup: V·∫´n fixed tr√™n translation02

**Advantages**:
‚úÖ Balance gi·ªØa control v√† flexibility  
‚úÖ Optimize resource usage  
‚úÖ MediaSoup stable v·ªõi UDP  

**Disadvantages**:
‚ö†Ô∏è Ph·ª©c t·∫°p h∆°n trong troubleshooting  

---

## üèóÔ∏è Implementation Plan - Option A (RECOMMENDED)

### Phase 1: Backup Current Stack

```bash
# Backup deployed service configs
ssh translation01 "docker service inspect translation_frontend > /tmp/frontend-backup.json"
ssh translation01 "docker service inspect translation_signaling > /tmp/signaling-backup.json"

# Backup file
cp infrastructure/swarm/stack-hybrid.yml infrastructure/swarm/stack-hybrid-backup-nov11.yml
```

### Phase 2: Update Stack Configuration

**File**: `infrastructure/swarm/stack-hybrid.yml`

**Changes**:

1. **Frontend service** (line ~280-340):
```yaml
frontend:
  # ... existing config ...
  deploy:
    # ... update config, rollback config, restart policy ...
    mode: replicated
    replicas: 3
    # ‚ö†Ô∏è B·ªé TO√ÄN B·ªò placement section
    # ƒê·ªÉ Swarm t·ª± ƒë·ªông ph√¢n ph·ªëi, s·ª≠ d·ª•ng ingress routing mesh
    resources:
      # ... existing limits ...
```

2. **Signaling service** (line ~60-100):
```yaml
signaling:
  # ... existing config ...
  deploy:
    # ... configs ...
    mode: replicated
    replicas: 3
    # ‚ö†Ô∏è B·ªé TO√ÄN B·ªò placement section  
    # ƒê·ªÉ Swarm ph√¢n ph·ªëi, c·∫£i thi·ªán HA
    resources:
      # ... existing limits ...
```

3. **MediaSoup service** (KEEP placement - c·∫ßn UDP mode: host):
```yaml
mediasoup:
  # ... existing config ...
  deploy:
    mode: replicated
    replicas: 1  
    placement:
      constraints:
        - node.labels.instance == translation02  # ‚úÖ KEEP THIS
    # ... rest of config ...
```

### Phase 3: Deploy Updated Stack

```bash
# SSH to manager node
ssh translation01

# Deploy updated stack
docker stack deploy -c /path/to/stack-hybrid.yml translation

# Monitor deployment
watch -n 2 "docker service ls | grep translation"

# Wait for convergence (~2-3 minutes)
```

### Phase 4: Verify Service Distribution

```bash
# Check frontend placement
docker service ps translation_frontend --format 'table {{.Name}}\t{{.Node}}\t{{.CurrentState}}'

# Expected: Replicas distributed across multiple nodes (translation01, translation02, translation03)

# Check signaling placement  
docker service ps translation_signaling --format 'table {{.Name}}\t{{.Node}}\t{{.CurrentState}}'

# Expected: Distributed across nodes

# Check mediasoup (should still be on translation02)
docker service ps translation_mediasoup --format 'table {{.Name}}\t{{.Node}}\t{{.CurrentState}}'

# Expected: 1 replica on translation02
```

### Phase 5: Test Frontend Accessibility

```bash
# Test 1: Basic HTTPS access
curl -I https://jbcalling.site/
# Expected: 200 OK or 30x redirect (NOT 504)

curl -I https://www.jbcalling.site/
# Expected: 200 OK or 30x redirect (NOT 504)

# Test 2: Full page load
curl -L https://jbcalling.site/ | grep -i "<!doctype"
# Expected: HTML response

# Test 3: API endpoint
curl -I https://api.jbcalling.site/health
# Expected: 200 OK

# Test 4: From external (your local machine)
curl -I https://jbcalling.site/
# Expected: 200 OK
```

### Phase 6: Verify Traefik Routing

```bash
# Check Traefik dashboard
curl -I https://traefik.jbcalling.site/dashboard/

# Check Traefik logs (should show successful routes)
docker service logs translation_traefik --tail 50 | grep -i frontend

# Expected: 200/30x responses, NO 504 errors

# Check Traefik discovered backends
docker service logs translation_traefik | grep -i "frontend@docker"
# Expected: Should show multiple backend IPs (distributed replicas)
```

### Phase 7: End-to-End WebRTC Test

1. **User 1**: M·ªü https://jbcalling.site
2. **Create Room**: T·∫°o room m·ªõi
3. **User 2**: Join room (incognito/kh√°c browser)
4. **Verify**:
   - Socket.IO connection: Check browser console ‚Üí `socket.connected = true`
   - ICE candidates: Should include media.jbcalling.site
   - Video/Audio streams: Should be flowing

### Phase 8: Monitor Logs During Test

```bash
# Terminal 1: Frontend logs
docker service logs translation_frontend --follow | grep -i "GET\|POST\|error"

# Terminal 2: Signaling logs
docker service logs translation_signaling --follow | grep -i "socket\|room\|error"

# Terminal 3: MediaSoup logs
docker service logs translation_mediasoup --follow | grep -i "ice\|dtls\|transport"

# Terminal 4: Traefik logs
docker service logs translation_traefik --follow | grep -i "frontend\|api"
```

---

## üìä Resource Planning

### Current State (with constraints)
```
translation01 (Manager, 30GB RAM):
  - Traefik, Redis, Translation, Monitoring
  - NO frontend

translation02 (Worker, 15GB RAM):  
  - MediaSoup (UDP mode: host) ‚úÖ STABLE
  - Coturn
  - NO frontend

translation03 (Worker, 15GB RAM):
  - Frontend (3 replicas) ‚Üê ALL HERE, UNREACHABLE
  - Signaling (3 replicas) ‚Üê ALL HERE
  - TTS
```

### Proposed State (Option A - no constraints)
```
translation01 (Manager, 30GB RAM):
  - Traefik ‚Üê INGRESS POINT
  - Redis, Translation, Monitoring
  - Frontend (1-2 replicas) ‚Üê AUTO DISTRIBUTED
  - Signaling (1-2 replicas) ‚Üê AUTO DISTRIBUTED

translation02 (Worker, 15GB RAM):
  - MediaSoup (1 replica) ‚úÖ FIXED placement
  - Coturn
  - Frontend (1 replica) ‚Üê AUTO DISTRIBUTED  
  - Signaling (1 replica) ‚Üê AUTO DISTRIBUTED

translation03 (Worker, 15GB RAM):
  - TTS
  - Frontend (1 replica) ‚Üê AUTO DISTRIBUTED
  - Signaling (1 replica) ‚Üê AUTO DISTRIBUTED
```

### Memory Estimates (per replica)
- Frontend: 128M limit, 64M reserved  
- Signaling: 512M limit, 256M reserved  
- MediaSoup: 1GB limit, 512M reserved  

### Balanced Distribution (3 replicas each)
```
translation01: Frontend + Signaling = ~640M + existing services
translation02: Frontend + Signaling + MediaSoup = ~1.6GB + existing  
translation03: Frontend + Signaling + TTS = ~640M + existing
```

**Verdict**: ‚úÖ All nodes c√≥ ƒë·ªß RAM cho distribution n√†y

---

## üîí Rollback Plan

N·∫øu Option A fails:

### Rollback Step 1: Restore Frontend Placement
```bash
ssh translation01 "docker service update \
  --constraint-add 'node.labels.instance==translation01' \
  translation_frontend"
```

**Rationale**: Co-locate frontend v·ªõi Traefik (Option B)

### Rollback Step 2: Full Stack Rollback
```bash
ssh translation01 "docker stack deploy -c /tmp/stack-hybrid-backup-nov11.yml translation"
```

### Rollback Step 3: Service-level Rollback
```bash
ssh translation01 "docker service rollback translation_frontend"
ssh translation01 "docker service rollback translation_signaling"
```

---

## ‚úÖ Success Criteria

1. ‚úÖ `curl -I https://jbcalling.site/` returns 200 OK (not 504)
2. ‚úÖ Frontend replicas distributed across multiple nodes
3. ‚úÖ Traefik logs show successful routing (no 504 errors)
4. ‚úÖ End-to-end WebRTC call works (2 users can video chat)
5. ‚úÖ Socket.IO connects to api.jbcalling.site successfully
6. ‚úÖ MediaSoup still stable on translation02
7. ‚úÖ All services healthy in `docker service ls`

---

## üìù Notes & Considerations

### Why Not Use `mode: global`?
- `mode: global` deploys 1 replica per node
- We want 3 replicas total (not necessarily 1 per node)
- `mode: replicated` with no constraint is more flexible

### Why Keep MediaSoup Placement?
- MediaSoup uses `mode: host` for UDP ports
- UDP port binding requires fixed node
- Published ports 40000-40009 on translation02
- Changing placement would break UDP connectivity

### Alternative: Use `traefik.docker.network`?
- Already tried: Added label, still 504
- Not the issue - problem is cross-node routing, not network selection

### Alternative: Use External Load Balancer?
- Google Cloud Load Balancer ‚Üí Traefik ‚Üí Services
- More complex, additional cost
- Overkill for 3-node cluster
- Keep as last resort if Swarm mesh fails

---

## üöÄ Next Steps

1. **Review this document** v·ªõi user ƒë·ªÉ confirm Option A
2. **Backup current configs** before changes
3. **Update stack-hybrid.yml** to remove placement constraints
4. **Deploy and test** following Phase 1-8 above
5. **Monitor and document** results in new wrap-up file

---

**End of Document**
