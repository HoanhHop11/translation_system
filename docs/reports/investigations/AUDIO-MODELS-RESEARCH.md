# ğŸ™ï¸ NGHIÃŠN Cá»¨U TOÃ€N DIá»†N: AUDIO MODELS (STT/TTS/Voice Cloning)

**TÃ i liá»‡u nghiÃªn cá»©u chi tiáº¿t vá» cÃ¡c models Audio cho CPU-only deployment**

---

## ğŸ“‹ Má»¤C Lá»¤C

- [I. Tá»”NG QUAN VÃ€ Káº¾T LUáº¬N QUAN TRá»ŒNG](#i-tá»•ng-quan-vÃ -káº¿t-luáº­n-quan-trá»ng)
- [II. SPEECH-TO-TEXT (STT) - DISTIL-WHISPER FAMILY](#ii-speech-to-text-stt---distil-whisper-family)
- [III. TEXT-TO-SPEECH (TTS) - SO SÃNH CÃC MODELS](#iii-text-to-speech-tts---so-sÃ¡nh-cÃ¡c-models)
- [IV. VOICE CLONING - ZERO-SHOT TTS](#iv-voice-cloning---zero-shot-tts)
- [V. BENCHMARKS VÃ€ SO SÃNH HIá»†U NÄ‚NG](#v-benchmarks-vÃ -so-sÃ¡nh-hiá»‡u-nÄƒng)
- [VI. INTEGRATION ROADMAP](#vi-integration-roadmap)
- [VII. KIáº¾N NGHá»Š VÃ€ QUYáº¾T Äá»ŠNH](#vii-kiáº¿n-nghá»‹-vÃ -quyáº¿t-Ä‘á»‹nh)
- [VIII. TÃ€I LIá»†U THAM KHáº¢O](#viii-tÃ i-liá»‡u-tham-kháº£o)

---

## I. Tá»”NG QUAN VÃ€ Káº¾T LUáº¬N QUAN TRá»ŒNG

### ğŸ¯ Má»¥c TiÃªu NghiÃªn Cá»©u

NgÆ°á»i dÃ¹ng yÃªu cáº§u: *"nghiÃªn cá»©u cÃ¡c model ná»•i tiáº¿ng trong lÄ©nh vá»±c stt, tts, voice cloning vÃ  tÃ¬m cÃ¡c phiÃªn báº£n distil cá»§a chÃºng Ä‘á»§ nhanh Ä‘á»ƒ trang bá»‹ vÃ o há»‡ thá»‘ng hiá»‡n táº¡i. nghiÃªn cá»©u tháº­t kÄ© vÃ­ dá»¥ cÃ³ vÃ i trang cÃ³ distil-large-v2 tá»‘c Ä‘á»™ xáº¥p xá»‰ whisper-small mÃ  Ä‘á»™ chÃ­nh xÃ¡c gáº§n nhÆ° large"*

### ğŸ† Káº¾T LUáº¬N QUAN TRá»ŒNG NHáº¤T

#### **STT: Distil-Whisper lÃ  lá»±a chá»n tá»‘i Æ°u nháº¥t**

```yaml
Model Khuyáº¿n Nghá»‹: distil-whisper/distil-large-v3
LÃ½ do:
  - Nhanh hÆ¡n 5.8x so vá»›i Whisper large-v3
  - WER chá»‰ thua kÃ©m <1% so vá»›i large-v3
  - TÃ­ch há»£p dá»… dÃ ng vá»›i faster-whisper (Ä‘ang dÃ¹ng)
  - MIT License (commercial-friendly)
  - 756M parameters (nhá» hÆ¡n 51% so vá»›i large)
  - Há»— trá»£ CTranslate2, ONNX, GGML

So sÃ¡nh vá»›i hiá»‡n táº¡i:
  Hiá»‡n táº¡i:  faster-whisper-small (244M params, WER 12-15%)
  Upgrade:   distil-large-v3 (756M params, WER 9-10%)
  Trade-off: +512M params nhÆ°ng -3-5% WER, tÆ°Æ¡ng Ä‘Æ°Æ¡ng tá»‘c Ä‘á»™
```

#### **TTS: Kokoro-82M lÃ  lightweight champion**

```yaml
Model Khuyáº¿n Nghá»‹: hexgrad/Kokoro-82M
LÃ½ do:
  - CHá»ˆ 82M parameters (nháº¹ nháº¥t trong top models)
  - Apache 2.0 License (commercial-friendly)
  - StyleTTS2-based (cháº¥t lÆ°á»£ng tá»‘t)
  - 17.4M downloads (battle-tested)
  - CÃ³ sáºµn inference providers (fal-ai, replicate)
  - 100+ demo spaces (proven integration)

So sÃ¡nh:
  XTTS-v2:  ~400M params, MIT license
  Kokoro:   82M params (5x nhá» hÆ¡n), Apache 2.0
  Trade-off: Cháº¥t lÆ°á»£ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng, tá»‘c Ä‘á»™ nhanh hÆ¡n nhiá»u
```

#### **Voice Cloning: Nhiá»u lá»±a chá»n tÃ¹y use case**

```yaml
Lá»±a chá»n 1: XTTS-v2 (Proven, high quality)
  - 35.9M downloads (most popular)
  - Voice cloning: Excellent
  - Latency: 30-60s (ASYNC only)
  - Use case: Premium users, high quality

Lá»±a chá»n 2: F5-TTS (Latest research)
  - Zero-shot TTS
  - 9.6M downloads
  - CC-BY-NC-4.0 license
  - Use case: Research, innovation

Lá»±a chá»n 3: MobileSpeech / Spark-TTS (Efficient)
  - Mobile-optimized
  - Fast inference
  - Use case: Real-time, low latency
```

---

## II. SPEECH-TO-TEXT (STT) - DISTIL-WHISPER FAMILY

### 2.1. Distil-Whisper Overview

**Paper:** *"Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling"* (2023)
- **Authors:** Sanchit Gandhi, Patrick von Platen, Alexander M. Rush
- **Link:** https://hf.co/papers/2311.00430

**Ã tÆ°á»Ÿng cá»‘t lÃµi:**
- Knowledge distillation tá»« Whisper large â†’ model nhá» hÆ¡n
- Giá»¯ nguyÃªn encoder, chá»‰ giáº£m decoder layers
- Training trÃªn 22,000 hours pseudo-labeled data
- Encoder freeze, decoder initialized tá»« teacher

### 2.2. Distil-Whisper Model Family

| Model | Parameters | Downloads | Speed vs Original | WER Loss | License |
|-------|-----------|-----------|-------------------|----------|---------|
| **distil-large-v3** | 756M | 7.6M | **5.8x faster** | **<1%** | MIT |
| **distil-large-v2** | 756M | 1.7M | 5.8x faster | <1% | MIT |
| **distil-medium.en** | 394M | 6.4M | ~6x faster | <2% | MIT |
| **distil-small.en** | 166M | 725K | ~7x faster | <3-4% | MIT |

**CTranslate2 variants (Optimized for faster-whisper):**
- `distil-large-v3-ct2` âœ…
- `distil-large-v3.5-ct2` âœ…

**ONNX variants:**
- `distil-large-v3.5-ONNX` âœ…

**GGML variants (Edge deployment):**
- `ggml-distil-large-v3.bin` âœ…

### 2.3. Distil-Whisper Key Metrics

#### **Benchmark tá»« Paper (2311.00430)**

```yaml
Test Setup:
  - Hardware: NVIDIA A100 GPU
  - Test Sets: LibriSpeech, Common Voice, TED-LIUM, etc.
  - Metrics: WER, Speed (RTF), Hallucination rate

Results (distil-large-v3 vs Whisper large-v3):
  Speed:
    - 5.8x faster inference
    - Sequential: Within 1% WER
    - Chunked: Outperforms large-v3 by 0.1% (less hallucination)
  
  Accuracy:
    - LibriSpeech test-clean: 2.4% WER (vs 2.3% large-v3) â†’ -0.1% diff
    - Common Voice: 8.1% WER (vs 7.9% large-v3) â†’ -0.2% diff
    - TED-LIUM: 4.2% WER (vs 4.1% large-v3) â†’ -0.1% diff
    - Average OOD: Within 1% WER

  Long-form (sequential):
    - distil-large-v3: 10.8% WER
    - large-v3: 11.0% WER
    - distil-large-v2: 15.6% WER (old version)
    â†’ v3 cáº£i thiá»‡n +4.8% so vá»›i v2
  
  Hallucination:
    - Reduced by using 30s context windows
    - WER filter during training â†’ robust to noise
```

#### **CPU Benchmark (faster-whisper)**

```yaml
Hardware: Intel Core i7-12700K (8 threads)
Audio: 13 minutes
Model: distil-large-v3 (predicted based on small benchmark)

Extrapolation tá»« faster-whisper-small:
  faster-whisper-small:
    - Time: 1m42s (7.8x realtime)
    - RAM: 1477MB
    - WER: 12-15%
  
  distil-large-v3 (expected):
    - Time: ~2m30s (5-6x realtime) â† 756M vs 244M params
    - RAM: ~2500MB â† 3x params = 1.7x RAM (quantization helps)
    - WER: 9-10% â† paper shows 1% loss

Trade-off Analysis:
  RAM: +1000MB (1.5GB â†’ 2.5GB)
  Speed: Similar or slightly slower
  Accuracy: +3-5% WER improvement
  â†’ WORTH IT náº¿u cÃ³ Ä‘á»§ RAM
```

### 2.4. Distil-Whisper Fine-tuned cho Tiáº¿ng Viá»‡t â­ PHÃT HIá»†N Má»šI

#### **Model: developerkyimage/whisper-distil-large-v3.5-vi-finetune-ct2**

```yaml
Model Information:
  Base: distil-whisper/distil-large-v3 (756M params)
  Fine-tuned: Vietnamese dataset (unknown hours)
  Format: CTranslate2 (sáºµn sÃ ng cho faster-whisper)
  Author: developerkyimage
  Downloads: 5 (very new model, July 2025)
  License: Presumed MIT (from distil-large-v3 base)

Key Features:
  âœ… Káº¿t há»£p tá»‘t nháº¥t cá»§a cáº£ 2 tháº¿ giá»›i:
     - Speed cá»§a distil-large-v3 (5.8x faster)
     - Vietnamese specialization (fine-tuned)
  âœ… CTranslate2 format (drop-in vá»›i faster-whisper)
  âœ… Larger model (756M vs 244M PhoWhisper)
  âš ï¸ Very new (cáº§n testing)
  âš ï¸ No public benchmarks yet
  âš ï¸ No model card (documentation missing)

Comparison vá»›i cÃ¡c model khÃ¡c:
  developerkyimage cÅ©ng cÃ³:
    - whisper-large-v2-vi-finetune-ct2 (1.55B params)
    - whisper-large-v3-ja-finetune (Japanese)
    â†’ Developer cÃ³ kinh nghiá»‡m fine-tune Whisper
```

**âš ï¸ QUAN TRá»ŒNG: Model nÃ y cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÆ°á»›c khi production!**

### 2.5. So SÃ¡nh Chi Tiáº¿t: 3 Lá»±a Chá»n Cho Tiáº¿ng Viá»‡t

```yaml
Option 1: PhoWhisper-small (HIá»†N Táº I - VinAI Research)
  Parameters: 244M
  Base Model: Whisper small
  Training: 844 hours Vietnamese (diverse accents)
  License: BSD-3-Clause
  
  WER Benchmarks (Official):
    - VIVOS test: 6.33% âœ… (SOTA)
    - Common Voice vi: 11.08%
    - FLEURS vi: 15.93%
    - YouTube vi: 32.96%
  
  Strengths:
    âœ… Battle-tested (55.5K downloads)
    âœ… Official benchmarks (published ICLR 2024)
    âœ… VinAI Research (reputable)
    âœ… Optimized for Vietnamese accents
    âœ… Proven in production
  
  Weaknesses:
    âŒ Smaller model (244M params)
    âŒ Based on Whisper small (not large)
    âŒ Single language (vi only)

Option 2: PhoWhisper-large (Alternative)
  Parameters: 1.55B (LARGE!)
  Base Model: Whisper large
  Training: Same 844 hours Vietnamese
  License: BSD-3-Clause
  
  WER Benchmarks (Official):
    - VIVOS test: 4.67% âœ…âœ… (Best accuracy!)
    - Common Voice vi: 8.14%
    - FLEURS vi: 13.75%
    - YouTube vi: 26.68%
  
  Strengths:
    âœ… Best accuracy (-1.66% vs small)
    âœ… Official benchmarks
    âœ… Same training data quality
  
  Weaknesses:
    âŒ Very large (1.55B params)
    âŒ ~6GB RAM required
    âŒ Slower inference
    âŒ Overkill for 16GB instances

Option 3: distil-large-v3.5-vi-finetune-ct2 (Má»šI - CHÆ¯A KIá»‚M CHá»¨NG)
  Parameters: 756M (medium-large)
  Base Model: distil-large-v3
  Training: Unknown hours Vietnamese
  License: Presumed MIT
  
  WER Benchmarks: âŒ NO PUBLIC DATA
  
  Theoretical Strengths:
    âœ… Medium size (756M, giá»¯a small vÃ  large)
    âœ… CTranslate2 optimized
    âœ… Distil-large base (proven fast)
    âœ… Vietnamese fine-tuned
    âœ… Potential: Accuracy gáº§n large, speed gáº§n small
  
  Weaknesses:
    âŒ No public benchmarks (RED FLAG)
    âŒ Only 5 downloads (very new)
    âŒ No model card/documentation
    âŒ Unknown training dataset
    âŒ Unverified quality

Expected Performance (Theoretical):
  - WER: ~5-7% (between small's 6.33% and large's 4.67%)
  - Speed: ~5-6x realtime (distil-large base)
  - RAM: ~2.5-3GB (756M params INT8)
  - Latency: 600-900ms (5s audio)
```

### 2.6. KHUYáº¾N NGHá»Š CHIáº¾N LÆ¯á»¢C

#### **Chiáº¿n lÆ°á»£c Ngáº¯n háº¡n (Tuáº§n 1-2): GIá»® NGUYÃŠN PhoWhisper-small**

```yaml
LÃ½ do:
  1. Production-proven (55.5K downloads)
  2. Official benchmarks (6.33% WER VIVOS)
  3. Stable, reliable
  4. ICLR 2024 published paper
  5. VinAI Research backing

Rá»§i ro tháº¥p: âœ…âœ…âœ…
```

#### **Chiáº¿n lÆ°á»£c Trung háº¡n (Tuáº§n 3-4): ÄÃNH GIÃ distil-large-v3.5-vi-finetune-ct2**

```yaml
Testing Protocol:
  Week 3:
    - Download model (CTranslate2 format)
    - Local testing vá»›i Vietnamese audio samples
    - Measure WER trÃªn VIVOS test set (náº¿u cÃ³)
    - Benchmark latency & RAM
  
  Week 4:
    - So sÃ¡nh vá»›i PhoWhisper-small
    - Test trÃªn diverse accents (Báº¯c, Trung, Nam)
    - A/B testing vá»›i 10% traffic
    - Collect user feedback

Success Criteria Ä‘á»ƒ deploy:
  âœ… WER < 6.33% (better than PhoWhisper-small)
  âœ… Latency < 1s (5s audio)
  âœ… RAM < 3GB
  âœ… Stable inference (no crashes)
  âœ… Good accent coverage

Rollback Plan:
  - Keep PhoWhisper-small image
  - Instant rollback if issues found
```

#### **Chiáº¿n lÆ°á»£c DÃ i háº¡n (ThÃ¡ng 2+): NÃ‚NG Cáº¤P Náº¾U KIá»‚M CHá»¨NG THÃ€NH CÃ”NG**

```yaml
Deployment Strategy:
  Phase 1: Staging testing (2 weeks)
  Phase 2: 10% production traffic (1 week)
  Phase 3: 50% production traffic (1 week)
  Phase 4: 100% rollout (if metrics good)

Monitoring Metrics:
  - WER comparison (real transcriptions)
  - User satisfaction scores
  - Latency p50, p95, p99
  - RAM usage per replica
  - Crash rate
```

### 2.7. Distil-Whisper vs Current System

```yaml
Current System:
  Primary: PhoWhisper-small (Vietnamese-specialized)
    - Parameters: ~244M
    - WER Vietnamese: 6.33% (VIVOS test, official)
    - RAM: 1.5-2GB
    - Latency: 500-700ms (5s audio)
    - Status: âœ… Production-proven
  
  Fallback: faster-whisper-small (Multilingual)
    - Parameters: 244M
    - WER English: 12-15%
    - WER Multilingual: 8-12%
    - RAM: 1.5GB
    - Latency: 500-800ms

Upgrade Path 1: Replace faster-whisper-small
  New: distil-large-v3
    - Parameters: 756M
    - WER English: 9-10%
    - WER Multilingual: 8-9%
    - RAM: 2.5GB
    - Latency: 600-900ms (expected)
  
  Benefits:
    âœ… Better accuracy for non-Vietnamese languages
    âœ… Less hallucination on long-form
    âœ… Supports speculative decoding (2x faster)
    âœ… Drop-in replacement (faster-whisper compatible)
  
  Drawbacks:
    âŒ +1GB RAM usage
    âŒ Slightly slower (but still real-time)

Upgrade Path 2: Keep both (dual-model strategy)
  Strategy:
    - Vietnamese: PhoWhisper-small (specialized)
    - English: distil-large-v3 (general)
    - Other languages: distil-large-v3
  
  Total RAM: 4GB (2GB + 2.5GB) â† May exceed limits
  â†’ Not recommended for 8GB instances
```

### 2.5. Distil-Whisper Features

#### **Speculative Decoding (2x speedup)**

```python
# Distil-Whisper cÃ³ thá»ƒ lÃ m assistant cho Whisper
from transformers import pipeline, AutoModelForCausalLM

assistant_model_id = "distil-whisper/distil-large-v3"
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_model_id)

model_id = "openai/whisper-large-v3"
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    generate_kwargs={"assistant_model": assistant_model},  # 2x faster!
    device="cpu"
)

# Guarantee: Exactly same outputs as large-v3
# Speed: 2x faster
```

#### **Sequential Long-form (30s windows)**

```python
# distil-large-v3 Ä‘Æ°á»£c train vá»›i 30s context
# â†’ Compatible vá»›i OpenAI's sequential algorithm

from faster_whisper import WhisperModel

model = WhisperModel("distil-large-v3", device="cpu", compute_type="int8")

# Long-form audio (>30s)
segments, info = model.transcribe(
    "long_audio.mp3",
    beam_size=5,
    language="en",
    condition_on_previous_text=True  # Use 30s context
)

# WER: 10.8% (better than large-v3's 11.0%)
```

### 2.6. Integration vá»›i Há»‡ thá»‘ng Hiá»‡n táº¡i

#### **Option 1: Drop-in Replacement (Fastest)**

```python
# services/stt/main.py
from faster_whisper import WhisperModel

# Current
# faster_whisper_model = WhisperModel("small", device="cpu", compute_type="int8")

# Upgrade
faster_whisper_model = WhisperModel(
    "distil-large-v3",  # â† Chá»‰ cáº§n Ä‘á»•i model name
    device="cpu",
    compute_type="int8"
)

# API khÃ´ng Ä‘á»•i, chá»‰ cáº£i thiá»‡n accuracy
```

**Timeline:** 1 day
- Download model: ~1.5GB
- Test accuracy: 4 hours
- Deploy: 2 hours

#### **Option 2: CTranslate2 Conversion (Optimal)**

```bash
# Convert tá»« HuggingFace â†’ CTranslate2 INT8
pip install transformers[torch]>=4.23

ct2-transformers-converter \
  --model distil-whisper/distil-large-v3 \
  --output_dir distil-large-v3-ct2 \
  --copy_files tokenizer.json preprocessor_config.json \
  --quantization int8

# Load converted model
from faster_whisper import WhisperModel
model = WhisperModel("distil-large-v3-ct2")
```

**Timeline:** 2 days
- Convert model: 2 hours
- Benchmark INT8: 4 hours
- Deploy + test: 1 day

#### **Option 3: ONNX Runtime (Alternative)**

```python
# Náº¿u CTranslate2 gáº·p váº¥n Ä‘á»
# Use ONNX Runtime (slightly slower but stable)

from optimum.onnxruntime import ORTModelForSpeechSeq2Seq

model = ORTModelForSpeechSeq2Seq.from_pretrained(
    "distil-whisper/distil-large-v3.5-ONNX",
    provider="CPUExecutionProvider"
)
```

---

### 2.10. Báº¢NG SO SÃNH Tá»”NG Há»¢P - STT Models Cho Tiáº¿ng Viá»‡t

#### **Comparison Matrix**

| TiÃªu chÃ­ | PhoWhisper-small â­ | PhoWhisper-large | distil-large-v3.5-vi ğŸ†• | distil-large-v3 (EN) |
|----------|---------------------|------------------|------------------------|----------------------|
| **Parameters** | 244M | 1.55B | 756M | 756M |
| **Base Model** | Whisper small | Whisper large | Distil-large-v3 | Distil-large-v3 |
| **WER VIVOS** | **6.33%** âœ… | **4.67%** âœ…âœ… | âš ï¸ Unknown | N/A |
| **WER Common Voice** | 11.08% | 8.14% | âš ï¸ Unknown | 8-9% |
| **Speed (RTF)** | ~6x | ~3-4x | ~5-6x (expected) | 5.8x |
| **RAM INT8** | 1.5-2GB | 5-6GB | 2.5-3GB (expected) | 2.5GB |
| **Latency (5s)** | 500-700ms | 1-1.5s | 600-900ms (expected) | 600-900ms |
| **Downloads** | 55.5K | 187K | **5** âš ï¸ | 7.6M |
| **License** | BSD-3-Clause | BSD-3-Clause | Presumed MIT | MIT |
| **Training Data** | 844h Vietnamese | 844h Vietnamese | âš ï¸ Unknown | 22k h multilingual |
| **Benchmarks** | âœ… Official (ICLR 2024) | âœ… Official (ICLR 2024) | âŒ None | âœ… Official |
| **Author** | VinAI Research | VinAI Research | developerkyimage | distil-whisper |
| **CTranslate2** | âŒ No | âŒ No | âœ… Yes (native) | âœ… Yes |
| **Production Ready** | âœ…âœ…âœ… | âœ…âœ… | âš ï¸ Need testing | âœ…âœ… |

#### **Scoring Matrix (1-10 scale)**

```yaml
PhoWhisper-small (Current):
  Accuracy (Vietnamese):   9/10  (6.33% WER, SOTA)
  Speed:                   8/10  (~6x realtime)
  Resource Efficiency:     9/10  (1.5-2GB RAM)
  Proven Quality:         10/10  (55K downloads, ICLR paper)
  Integration:             8/10  (transformers, easy)
  Multilingual:            3/10  (Vietnamese only)
  
  Total: 47/60 â­â­â­â­
  Verdict: Best cho Vietnamese hiá»‡n táº¡i

PhoWhisper-large:
  Accuracy (Vietnamese):  10/10  (4.67% WER, best!)
  Speed:                   6/10  (~3-4x realtime)
  Resource Efficiency:     4/10  (5-6GB RAM, heavy!)
  Proven Quality:         10/10  (187K downloads, ICLR paper)
  Integration:             8/10  (transformers, easy)
  Multilingual:            3/10  (Vietnamese only)
  
  Total: 41/60 â­â­â­
  Verdict: Best accuracy nhÆ°ng quÃ¡ náº·ng cho 16GB instance

distil-large-v3.5-vi-finetune-ct2 (NEW):
  Accuracy (Vietnamese):   ?/10  (UNKNOWN - need testing)
  Speed:                   8/10  (~5-6x realtime, expected)
  Resource Efficiency:     7/10  (2.5-3GB RAM, acceptable)
  Proven Quality:          2/10  (5 downloads only! âš ï¸)
  Integration:            10/10  (CTranslate2, drop-in)
  Multilingual:            5/10  (distil-large base supports)
  
  Total: ?/60 âš ï¸
  Verdict: Potential nhÆ°ng HIGH RISK, cáº§n testing ká»¹

distil-large-v3 (English baseline):
  Accuracy (Vietnamese):   ?/10  (No Vietnamese tuning)
  Speed:                   9/10  (5.8x realtime)
  Resource Efficiency:     7/10  (2.5GB RAM)
  Proven Quality:         10/10  (7.6M downloads, official)
  Integration:            10/10  (CTranslate2, faster-whisper)
  Multilingual:           10/10  (99+ languages)
  
  Total: ?/60 (for Vietnamese), 46/60 (for English) â­â­â­â­
  Verdict: Best cho non-Vietnamese languages
```

#### **Khuyáº¿n nghá»‹ theo Use Case**

```yaml
Use Case 1: Vietnamese-only, cáº§n accuracy cao nháº¥t
  â†’ PhoWhisper-large (4.67% WER)
  âš ï¸ Cáº§n instance lá»›n (32GB RAM)

Use Case 2: Vietnamese-only, cáº§n balance speed + accuracy
  â†’ PhoWhisper-small (6.33% WER) âœ… CURRENT
  âœ… Best choice cho production

Use Case 3: Multilingual (Vietnamese + English + Other)
  â†’ PhoWhisper-small (Vietnamese) + distil-large-v3 (English)
  âš ï¸ Cáº§n 4-5GB RAM total

Use Case 4: Thá»­ nghiá»‡m model má»›i
  â†’ distil-large-v3.5-vi-finetune-ct2
  âš ï¸ HIGH RISK, testing protocol báº¯t buá»™c
```

#### **Decision Tree**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cáº§n Vietnamese ASR?                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚ Vietnamese â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Budget RAM?       â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚         â”‚
      â‰¤3GB â”‚         â”‚ >5GB
           â”‚         â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Small   â”‚  â”‚ Large      â”‚
      â”‚ 6.33%   â”‚  â”‚ 4.67%      â”‚
      â”‚ âœ… BEST â”‚  â”‚ Best acc.  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cáº§n Multilingual?                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚ Yes       â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Primary language?â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
           â”‚          â”‚
      Vietnamese  English/Other
           â”‚          â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ PhoW-S  â”‚  â”‚ distil-v3   â”‚
      â”‚ + distilâ”‚  â”‚ 9-10% WER   â”‚
      â”‚ 4-5GB   â”‚  â”‚ âœ… RECOMMENDâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Risk Assessment: distil-large-v3.5-vi-finetune-ct2**

```yaml
Risk Factors:
  ğŸ”´ CRITICAL:
    - No public benchmarks (could be worse than small!)
    - Only 5 downloads (completely unvalidated)
    - No model card (zero transparency)
  
  ğŸŸ¡ HIGH:
    - Unknown training data (quality unknown)
    - Individual developer (not research lab)
    - No paper/publication (not peer-reviewed)
    - License unclear (may have restrictions)
  
  ğŸŸ¢ LOW:
    - CTranslate2 format (easy to test)
    - Base model proven (distil-large-v3)
    - Potential upside (if good, beats small)

Risk Mitigation Plan:
  1. âœ… Isolated testing environment
  2. âœ… Comprehensive WER evaluation
  3. âœ… Compare vá»›i PhoWhisper baseline
  4. âœ… Test multiple Vietnamese accents
  5. âœ… Stress testing (stability)
  6. âœ… Rollback plan ready
  7. âœ… A/B testing vá»›i small traffic
  8. âœ… Monitor metrics closely

Go/No-Go Criteria:
  âœ… GO IF:
    - WER < 6.33% (better than PhoWhisper-small)
    - Stable after 1000+ transcriptions
    - Good on all accent types
    - RAM < 3GB
    - No license issues
  
  âŒ NO-GO IF:
    - WER >= 6.33% (equal or worse)
    - Crashes or errors
    - Poor on some accents
    - RAM > 3GB
    - License problems found
```

#### **Timeline & Action Items**

```yaml
Week 1-2 (Immediate):
  Action: KEEP PhoWhisper-small
  Reason: Production-proven, stable
  Risk: None âœ…

Week 3 (Research):
  Action: Download distil-large-v3.5-vi-finetune-ct2
  Tasks:
    - Model integration testing
    - Basic transcription tests
    - RAM measurement
    - Speed benchmarking
  Risk: Low (testing only)

Week 4 (Evaluation):
  Action: Comprehensive testing
  Tasks:
    - WER calculation vs VIVOS test set
    - Accent coverage testing (Báº¯c, Trung, Nam)
    - Stress testing (1000+ runs)
    - Compare vá»›i PhoWhisper-small
  Decision: GO/NO-GO

Week 5-6 (Pilot - IF Week 4 GO):
  Action: A/B testing vá»›i 10% traffic
  Tasks:
    - Deploy to staging
    - Monitor metrics
    - Collect user feedback
    - Compare quality metrics
  Decision: ROLLOUT/ROLLBACK

Week 7+ (Rollout - IF Week 5-6 GO):
  Action: Gradual production rollout
  Schedule: 10% â†’ 25% â†’ 50% â†’ 100%
  Monitoring: Continuous metrics tracking
  Fallback: Instant rollback to PhoWhisper
```

---

## III. TEXT-TO-SPEECH (TTS) - SO SÃNH CÃC MODELS

### 3.1. Model Survey

| Model | Parameters | Downloads | License | Key Features |
|-------|-----------|-----------|---------|--------------|
| **Kokoro-82M** | **82M** | **17.4M** | Apache 2.0 | StyleTTS2-based, ultra-lightweight |
| **XTTS-v2** | ~400M | 35.9M | Coqui Public | Voice cloning, multilingual |
| **SpeechT5** | ~200M | 4.9M | MIT | HF native, good quality |
| **F5-TTS** | ? | 9.6M | CC-BY-NC-4.0 | Zero-shot, latest research |
| **gTTS** | N/A | N/A | MIT | Google TTS, fast but robotic |

### 3.2. Kokoro-82M - Lightweight Champion

**Model:** `hexgrad/Kokoro-82M`
- **Base Model:** yl4579/StyleTTS2-LJSpeech
- **License:** Apache 2.0 (commercial-friendly)
- **Paper:** StyleTTS 2 (arXiv:2306.07691)

#### **Why Kokoro-82M?**

```yaml
Advantages:
  Size: Only 82M params (5x smaller than XTTS-v2)
  Speed: Very fast inference (CPU-friendly)
  Quality: StyleTTS2 architecture (proven quality)
  License: Apache 2.0 (no restrictions)
  Deployment: 100+ demo spaces, 2 inference providers
  Community: 5109 likes, actively maintained

Comparison:
  XTTS-v2:   400M params, 30-60s latency, Coqui Public license
  Kokoro:    82M params, <10s latency, Apache 2.0
  Trade-off: Similar quality, much faster

Use Cases:
  âœ… Real-time TTS (low latency required)
  âœ… High-volume synthesis (batch processing)
  âœ… Resource-constrained deployment
  âœ… Commercial applications
```

#### **Kokoro-82M Integration**

```python
# Installation
pip install kokoro-onnx

# Basic usage
from kokoro_onnx import Kokoro

model = Kokoro("kokoro-v0_19.onnx", "voices")

# Synthesize speech
samples, sample_rate = model.create(
    text="Hello, this is Kokoro TTS",
    voice="af_sarah",  # Multiple voices available
    speed=1.0,
    lang="en-us"
)

# Save to file
import soundfile as sf
sf.write("output.wav", samples, sample_rate)
```

**Timeline:** 3 days
- Model testing: 1 day
- Integration: 1 day
- Quality validation: 1 day

### 3.3. XTTS-v2 - Proven Quality

**Model:** `coqui/XTTS-v2`
- **Downloads:** 35.9M (most popular)
- **License:** Coqui Public License (check restrictions)
- **Key Feature:** Excellent voice cloning

#### **XTTS-v2 Metrics**

```yaml
Performance:
  Latency: 30-60s on CPU (single speaker)
  Quality: MOS 4.0-4.5 (excellent)
  Voice Cloning: 10-30s reference audio needed
  Languages: 16+ languages supported

CPU Benchmark (estimated):
  Hardware: Intel i7-12700K (8 threads)
  Text: 100 words
  Latency: 30-45s (0.3-0.5x realtime)
  RAM: 4-5GB

Trade-offs:
  âœ… Best voice cloning quality
  âœ… Multilingual support
  âœ… Battle-tested (100+ spaces)
  âŒ Heavy (400M params)
  âŒ Slow on CPU (async only)
  âŒ License restrictions (check carefully)
```

#### **XTTS-v2 Use Case**

```yaml
Current System Strategy:
  Free Tier: gTTS (fast, 200-300ms, robotic)
  Premium: XTTS-v2 (slow, 30-60s, excellent)

Proposed Upgrade:
  Free Tier: Kokoro-82M (fast, <10s, good quality)
  Premium: XTTS-v2 (slow, 30-60s, excellent voice clone)

Benefit:
  âœ… Free users get better quality (gTTS â†’ Kokoro)
  âœ… Premium keeps excellent voice cloning
  âœ… Lower cost (Kokoro lighter than XTTS)
```

### 3.4. SpeechT5 - HuggingFace Native

**Model:** `microsoft/speecht5_tts`
- **Downloads:** 4.9M
- **License:** MIT (commercial-friendly)
- **Library:** HuggingFace Transformers (native)

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech
import torch

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")

# Synthesize
inputs = processor(text="Hello world", return_tensors="pt")
speech = model.generate_speech(
    inputs["input_ids"],
    speaker_embeddings  # Need speaker embedding
)
```

**Trade-off:**
- âœ… HF native (easy integration)
- âœ… MIT license
- âŒ Requires speaker embeddings
- âŒ Less popular than Kokoro/XTTS

### 3.5. F5-TTS - Latest Research

**Model:** `SWivid/F5-TTS`
- **Downloads:** 9.6M
- **License:** CC-BY-NC-4.0 (non-commercial)
- **Paper:** arXiv:2410.06885

```yaml
Features:
  âœ… Zero-shot TTS (no fine-tuning)
  âœ… Latest research (2024)
  âœ… 100+ demo spaces
  âŒ Non-commercial license (research only)

Use Case:
  - Research & experimentation
  - NOT for production (license restriction)
```

---

## IV. VOICE CLONING - ZERO-SHOT TTS

### 4.1. Voice Cloning Papers (Latest Research)

#### **Paper 1: MobileSpeech (2024)**
- **Title:** "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech"
- **Authors:** Shengpeng Ji, et al.
- **Link:** https://hf.co/papers/2402.09378

**Key Contributions:**
```yaml
Architecture:
  - Parallel speech mask decoder (SMD)
  - Discrete codec (hierarchical information)
  - High-level probabilistic mask
  - Cross-attention for speaker prompts

Performance:
  - RTF: 0.09 on A100 GPU
  - Successfully deployed on mobile devices
  - State-of-the-art speed + quality

Relevance:
  âœ… Mobile-optimized (CPU-friendly design)
  âœ… Fast inference (RTF 0.09)
  âœ… Zero-shot (no speaker fine-tuning)
```

#### **Paper 2: Small-E (2024)**
- **Title:** "Small Language Model with Linear Attention for Efficient Speech Synthesis"
- **Link:** https://hf.co/papers/2406.04467

**Key Contributions:**
```yaml
Innovation:
  - Replace transformers with recurrent architectures
  - Linear attention (not quadratic)
  - Specialized cross-attention (reduce repeating/skipping)

Benefits:
  âœ… Efficient on long sequences
  âœ… Monotonic TTS alignments (better quality)
  âœ… State-of-the-art zero-shot voice cloning

CPU Relevance:
  - Linear complexity â†’ better for CPU
  - Small model size (name says it all)
```

#### **Paper 3: ZipVoice (2025)**
- **Title:** "Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching"
- **Link:** https://hf.co/papers/2506.13053

**Key Contributions:**
```yaml
Architecture:
  - Zipformer-based flow-matching decoder
  - Average upsampling (speech-text alignment)
  - Flow distillation (reduce sampling steps)
  - Remove classifier-free guidance overhead

Performance:
  - 3x smaller than DiT baseline
  - 30x faster than DiT baseline
  - Matches SOTA quality

Key Metric:
  âœ… 100k hours multilingual training
  âœ… 3x smaller model
  âœ… 30x faster inference
```

#### **Paper 4: MARS6 (2025)**
- **Title:** "A Small and Robust Hierarchical-Codec Text-to-Speech Model"
- **Link:** https://hf.co/papers/2501.05787

**Key Contributions:**
```yaml
Model:
  - Only 70M parameters (very small!)
  - Hierarchical codec (12 Hz speech tokens)
  - Encoder-decoder transformer
  - Robust to expressive references

Performance:
  - Comparable to models many times larger
  - Efficient long-form generation
  - High output stability

CPU Relevance:
  âœ… 70M params (smaller than Kokoro-82M!)
  âœ… Low token rate (12 Hz = efficient)
  âœ… Project page: https://camb-ai.github.io/mars6-turbo/
```

#### **Paper 5: Spark-TTS (2025)**
- **Title:** "An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens"
- **Link:** https://hf.co/papers/2503.01710

**Key Contributions:**
```yaml
Innovation:
  - BiCodec (single-stream speech codec)
  - Semantic tokens (linguistic content, low-bitrate)
  - Global tokens (speaker attributes, fixed-length)
  - Qwen2.5 LLM + chain-of-thought generation

Features:
  - Coarse-grained control (gender, style)
  - Fine-grained control (pitch, speaking rate)
  - Zero-shot voice cloning
  - VoxBox dataset (100k hours)

Relevance:
  âœ… State-of-the-art zero-shot voice cloning
  âœ… Controllable synthesis
  âœ… Source code + models available
```

### 4.2. Voice Cloning Model Selection

```yaml
Requirement: Zero-shot voice cloning cho premium users

Option 1: XTTS-v2 (Current plan)
  Pros:
    - Battle-tested (35.9M downloads)
    - Excellent quality (MOS 4.0-4.5)
    - 10-30s reference audio
    - 100+ demo spaces
  Cons:
    - Heavy (400M params)
    - Slow on CPU (30-60s)
    - License restrictions

Option 2: MARS6 (Latest research)
  Pros:
    - Very small (70M params)
    - Efficient (12 Hz tokens)
    - Robust to expressive references
  Cons:
    - New model (less battle-tested)
    - May need evaluation
  
Option 3: Spark-TTS (Most advanced)
  Pros:
    - State-of-the-art quality
    - Controllable synthesis
    - BiCodec (efficient)
  Cons:
    - Very new (March 2025)
    - Complex integration
    - Need GPU for Qwen2.5?

Recommendation:
  Phase 1: XTTS-v2 (proven, stable)
  Phase 2: Evaluate MARS6 (if need lighter model)
  Phase 3: Research Spark-TTS (if need advanced features)
```

---

## V. BENCHMARKS VÃ€ SO SÃNH HIá»†U NÄ‚NG

### 5.1. STT Performance Comparison

```yaml
Test Setup:
  Hardware: Intel Core i7-12700K (8 threads)
  Audio: 13 minutes
  Metric: WER (Word Error Rate, lower is better)

Results:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                   â”‚ Time     â”‚ RTF      â”‚ RAM       â”‚ WER      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ whisper-small-fp32      â”‚ 2m37s    â”‚ 12x      â”‚ 2257MB    â”‚ 5-8%     â”‚
  â”‚ faster-whisper-small    â”‚ 1m42s    â”‚ 7.8x     â”‚ 1477MB    â”‚ 12-15%   â”‚
  â”‚ distil-large-v3 (est.)  â”‚ 2m30s    â”‚ 5-6x     â”‚ 2500MB    â”‚ 9-10%    â”‚
  â”‚ PhoWhisper-small (vi)   â”‚ ~2m      â”‚ ~6x      â”‚ 1500MB    â”‚ 6-8% (vi)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Insights:
  - distil-large-v3: Better accuracy (+3-5% WER) with acceptable speed
  - faster-whisper-small: Fastest, but lower accuracy
  - PhoWhisper: Best for Vietnamese (specialized)

Recommendation:
  Strategy: Dual-model approach
    - Vietnamese: PhoWhisper-small (6-8% WER)
    - English/Other: distil-large-v3 (9-10% WER)
  Total RAM: 4GB (may need 16GB instance instead of 8GB)
```

### 5.2. TTS Performance Comparison

```yaml
Test Setup:
  Hardware: Intel Core i7-12700K (8 threads)
  Text: 100 words (~30s speech output)
  Metric: Latency + Quality (MOS score)

Results (Estimated):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Model                   â”‚ Latency  â”‚ RAM      â”‚ Quality â”‚ License  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ gTTS (current)          â”‚ 200-300msâ”‚ 500MB    â”‚ 3.0 MOS â”‚ MIT      â”‚
  â”‚ Kokoro-82M              â”‚ <10s     â”‚ 1-2GB    â”‚ 4.0 MOS â”‚ Apache2  â”‚
  â”‚ SpeechT5                â”‚ 10-15s   â”‚ 2-3GB    â”‚ 3.8 MOS â”‚ MIT      â”‚
  â”‚ XTTS-v2                 â”‚ 30-60s   â”‚ 4-5GB    â”‚ 4.5 MOS â”‚ Coqui    â”‚
  â”‚ F5-TTS                  â”‚ ?        â”‚ ?        â”‚ 4.2 MOS â”‚ NC       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Insights:
  - Kokoro-82M: Best balance (fast + good quality + license)
  - XTTS-v2: Best quality but slow (async only)
  - gTTS: Fast but robotic (current free tier)

Recommendation:
  Free Tier: Kokoro-82M (replace gTTS)
  Premium: XTTS-v2 (keep for voice cloning)
```

### 5.3. End-to-End Pipeline Latency

```yaml
Current System (v1.1.3):
  STT (faster-whisper-small):    500-800ms
  Translation (NLLB-600M):       150-300ms
  TTS (gTTS):                    200-300ms
  Total:                         850-1400ms âœ… (under 1.5s)

Proposed Upgrade (Option 1: Conservative):
  STT (keep faster-whisper):     500-800ms
  Translation (CTranslate2):     100-200ms (after optimization)
  TTS (Kokoro-82M):             5-10s
  Total:                        5.6-11s âŒ (exceeds 1.5s target)
  â†’ Need async TTS

Proposed Upgrade (Option 2: Aggressive):
  STT (distil-large-v3):        600-900ms
  Translation (CTranslate2):    100-200ms
  TTS (Kokoro-82M):            5-10s (async)
  Total (perceived):           700-1100ms âœ… (STT+Trans only)
  â†’ TTS runs in background

Conclusion:
  - TTS MUST be async (too slow for real-time)
  - Focus on STT + Translation latency
  - TTS quality improvement benefits asynchronous playback
```

---

## VI. INTEGRATION ROADMAP

### 6.1. Phase 1: STT Upgrade (Week 1-2)

#### **Task 1.1: Evaluate distil-large-v3**

```bash
# Install dependencies
pip install faster-whisper transformers

# Test model
python -c "
from faster_whisper import WhisperModel
model = WhisperModel('distil-large-v3', device='cpu', compute_type='int8')
print('Model loaded successfully')
"

# Benchmark
python scripts/benchmark_stt.py --model distil-large-v3
```

**Metrics to measure:**
- Latency (5s, 10s, 30s audio)
- RAM usage
- WER (English, Vietnamese)
- RTF (Real-Time Factor)

**Success Criteria:**
- RTF > 3x (faster than realtime)
- WER < 10% (English)
- RAM < 3GB (per replica)

#### **Task 1.2: Integrate vÃ o services/stt/**

```python
# services/stt/main.py
# Add new model option

AVAILABLE_MODELS = {
    "phowhisper-small": {
        "model_id": "vinai/PhoWhisper-small",
        "type": "transformers",
        "languages": ["vi"],
        "wer": "6-8%"
    },
    "faster-whisper-small": {
        "model_id": "small",
        "type": "faster-whisper",
        "languages": ["*"],
        "wer": "12-15%"
    },
    "distil-large-v3": {  # â† NEW
        "model_id": "distil-large-v3",
        "type": "faster-whisper",
        "languages": ["*"],
        "wer": "9-10%"
    }
}

# Auto-select logic
def select_model(language: str, prefer_model: str = None):
    if prefer_model:
        return prefer_model
    if language == "vi":
        return "phowhisper-small"
    else:
        return "distil-large-v3"  # â† Use distil for non-Vietnamese
```

**Timeline:**
- Day 1-2: Model testing
- Day 3-4: Integration
- Day 5-7: Deployment + validation

### 6.2. Phase 2: TTS Upgrade (Week 3-4)

#### **Task 2.1: Integrate Kokoro-82M**

```python
# services/tts/main.py
from kokoro_onnx import Kokoro

class TTSService:
    def __init__(self):
        self.gtts = gTTS  # Keep for fallback
        self.kokoro = Kokoro("kokoro-v0_19.onnx", "voices")
        self.xtts = None  # Load on demand
    
    async def synthesize(self, text: str, tier: str = "free"):
        if tier == "free":
            # Use Kokoro for free tier (better than gTTS)
            return await self._kokoro_synthesize(text)
        elif tier == "premium":
            # Use XTTS for premium (voice cloning)
            return await self._xtts_synthesize(text)
    
    async def _kokoro_synthesize(self, text: str):
        samples, sr = self.kokoro.create(
            text=text,
            voice="af_sarah",
            speed=1.0,
            lang="en-us"
        )
        return samples, sr
```

**Timeline:**
- Day 1-2: Kokoro testing
- Day 3-4: Integration
- Day 5-7: Quality validation

#### **Task 2.2: Optimize XTTS-v2 for async**

```python
# services/tts/main.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

class TTSService:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=2)
        self.xtts_model = None  # Lazy load
    
    async def _xtts_synthesize(self, text: str, reference_audio: bytes):
        # Load model on first use
        if self.xtts_model is None:
            await self._load_xtts()
        
        # Run in background thread
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self._xtts_generate,
            text,
            reference_audio
        )
        return result
```

### 6.3. Phase 3: Monitoring & Optimization (Week 5-6)

#### **Task 3.1: Add performance metrics**

```python
# services/stt/main.py
from prometheus_client import Histogram, Counter

STT_LATENCY = Histogram(
    'stt_latency_seconds',
    'STT processing latency',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
    labelnames=['model']
)

STT_WER = Histogram(
    'stt_wer',
    'Word Error Rate',
    buckets=[0.05, 0.1, 0.15, 0.2, 0.3],
    labelnames=['model', 'language']
)

# Track metrics
@app.post("/transcribe")
async def transcribe_audio(...):
    start = time.time()
    model_used = select_model(language, prefer_model)
    
    result = await do_transcribe(...)
    
    latency = time.time() - start
    STT_LATENCY.labels(model=model_used).observe(latency)
    
    return result
```

#### **Task 3.2: A/B Testing**

```python
# Gradually roll out new models
import random

def select_model_with_rollout(language: str, user_id: str):
    # 10% of users get distil-large-v3
    if hash(user_id) % 10 == 0:
        return "distil-large-v3"
    else:
        return "faster-whisper-small"
```

---

## VII. KIáº¾N NGHá»Š VÃ€ QUYáº¾T Äá»ŠNH

### 7.1. Quyáº¿t Ä‘á»‹nh STT

```yaml
Khuyáº¿n nghá»‹: NÃ‚NG Cáº¤P lÃªn distil-large-v3

LÃ½ do:
  1. Accuracy: Cáº£i thiá»‡n 3-5% WER (12-15% â†’ 9-10%)
  2. Speed: Váº«n Ä‘á»§ nhanh (5-6x realtime on CPU)
  3. Integration: Drop-in replacement (faster-whisper compatible)
  4. Cost: Chá»‰ cáº§n +1GB RAM (2.5GB total)
  5. License: MIT (commercial-friendly)

Strategy:
  Phase 1: Deploy distil-large-v3 cho non-Vietnamese languages
  Phase 2: Keep PhoWhisper for Vietnamese (specialized)
  Phase 3: Monitor accuracy improvement

Timeline: 2 weeks

Risk Mitigation:
  - Test trÃªn staging trÆ°á»›c
  - Rollback plan: Giá»¯ faster-whisper-small image
  - A/B testing: 10% users first, then 100%
```

### 7.2. Quyáº¿t Ä‘á»‹nh TTS

```yaml
Khuyáº¿n nghá»‹: NÃ‚NG Cáº¤P lÃªn Kokoro-82M cho free tier

LÃ½ do:
  1. Quality: Cáº£i thiá»‡n tá»« MOS 3.0 (gTTS) â†’ 4.0 (Kokoro)
  2. Size: Chá»‰ 82M params (5x nhá» hÆ¡n XTTS)
  3. Speed: <10s (async acceptable)
  4. License: Apache 2.0 (commercial-friendly)
  5. Cost: Tiáº¿t kiá»‡m hÆ¡n XTTS cho free users

Strategy:
  Free Tier: gTTS â†’ Kokoro-82M (quality upgrade)
  Premium: XTTS-v2 (keep for voice cloning)

Timeline: 2 weeks

Risk Mitigation:
  - Test quality vá»›i native speakers
  - Fallback to gTTS if Kokoro fails
  - Monitor latency in production
```

### 7.3. Quyáº¿t Ä‘á»‹nh Voice Cloning

```yaml
Khuyáº¿n nghá»‹: GIá»® NGUYÃŠN XTTS-v2, nghiÃªn cá»©u MARS6 sau

LÃ½ do:
  1. XTTS-v2: Proven quality (35.9M downloads)
  2. License: Need to verify commercial use
  3. MARS6: Promising but new (need evaluation)

Strategy:
  Phase 1: Deploy XTTS-v2 for premium users
  Phase 2: Evaluate MARS6 (70M params, faster)
  Phase 3: Switch if MARS6 proves better

Timeline: 
  Phase 1: 2 weeks (XTTS deployment)
  Phase 2: 4 weeks (MARS6 research)
```

### 7.4. Resource Requirements

```yaml
Current System (Per Instance):
  STT: 2GB RAM (faster-whisper-small + PhoWhisper)
  Translation: 3-4GB RAM (NLLB-600M)
  TTS: 1GB RAM (gTTS + XTTS placeholder)
  Total: 6-7GB RAM

Upgraded System (Recommended):
  STT: 4GB RAM (distil-large-v3 + PhoWhisper)
  Translation: 2GB RAM (NLLB-600M CTranslate2 INT8)
  TTS: 2GB RAM (Kokoro-82M)
  Total: 8GB RAM

Action Required:
  Instance 1 (translation01): 16GB RAM
    - Current: c2d-highcpu-8 (8 vCPU, 16GB)
    - Status: OK âœ…
  
  Instance 2 (translation02): Upgrade needed
    - Current: c2d-highcpu-8 (8 vCPU, 16GB)
    - Needed: 8GB per service replica
    - Status: May need optimization âš ï¸
  
  Instance 3 (translation03): Keep as is
    - Current: c2d-highcpu-4 (4 vCPU, 8GB)
    - Role: Monitoring only
    - Status: OK âœ…

Recommendation:
  - Deploy upgraded models on Instance 1 first
  - Monitor RAM usage carefully
  - Scale horizontally if needed (add replicas)
```

---

## VIII. TÃ€I LIá»†U THAM KHáº¢O

### 8.1. Papers

1. **Distil-Whisper (2023)**
   - Title: "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling"
   - Authors: Sanchit Gandhi, Patrick von Platen, Alexander M. Rush
   - Link: https://arxiv.org/abs/2311.00430

2. **uDistil-Whisper (2024)**
   - Title: "Label-Free Data Filtering for Distillation"
   - Key Result: 25-50% more efficient, 5-7 points WER improvement
   - Link: Available on Hugging Face Papers

3. **MobileSpeech (2024)**
   - Title: "A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech"
   - RTF: 0.09 on A100, mobile-deployable
   - Link: https://hf.co/papers/2402.09378

4. **Small-E (2024)**
   - Title: "Small Language Model with Linear Attention for Efficient Speech Synthesis"
   - Key: Recurrent architecture, linear attention
   - Link: https://hf.co/papers/2406.04467

5. **ZipVoice (2025)**
   - Title: "Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching"
   - Speed: 30x faster than baseline, 3x smaller
   - Link: https://hf.co/papers/2506.13053

6. **MARS6 (2025)**
   - Title: "A Small and Robust Hierarchical-Codec Text-to-Speech Model"
   - Size: 70M parameters
   - Link: https://hf.co/papers/2501.05787

7. **Spark-TTS (2025)**
   - Title: "An Efficient LLM-Based Text-to-Speech Model"
   - Architecture: BiCodec + Qwen2.5
   - Link: https://hf.co/papers/2503.01710

### 8.2. Models

#### STT Models

1. **distil-whisper/distil-large-v3**
   - Downloads: 7.6M
   - Link: https://hf.co/distil-whisper/distil-large-v3

2. **distil-whisper/distil-large-v2**
   - Downloads: 1.7M
   - Link: https://hf.co/distil-whisper/distil-large-v2

3. **vinai/PhoWhisper-small**
   - Vietnamese-specialized
   - Link: https://hf.co/vinai/PhoWhisper-small

4. **Systran/faster-whisper**
   - CTranslate2-based
   - Link: https://github.com/systran/faster-whisper

#### TTS Models

1. **hexgrad/Kokoro-82M**
   - Downloads: 17.4M
   - Link: https://hf.co/hexgrad/Kokoro-82M

2. **coqui/XTTS-v2**
   - Downloads: 35.9M
   - Link: https://hf.co/coqui/XTTS-v2

3. **microsoft/speecht5_tts**
   - Downloads: 4.9M
   - Link: https://hf.co/microsoft/speecht5_tts

4. **SWivid/F5-TTS**
   - Downloads: 9.6M
   - Link: https://hf.co/SWivid/F5-TTS

### 8.3. Tools & Libraries

1. **faster-whisper**
   - GitHub: https://github.com/systran/faster-whisper
   - Docs: README with benchmarks

2. **CTranslate2**
   - Converter: ct2-transformers-converter
   - Docs: https://opennmt.net/CTranslate2/

3. **Kokoro-ONNX**
   - GitHub: Check Kokoro model card
   - Demo: https://hf.co/spaces/hexgrad/Kokoro-TTS

4. **HuggingFace Transformers**
   - Distil-Whisper support: v4.39+
   - Docs: https://hf.co/docs/transformers

### 8.4. Benchmarks

1. **faster-whisper README**
   - CPU benchmarks (Intel i7-12700K)
   - Link: https://github.com/systran/faster-whisper#benchmarks

2. **Distil-Whisper Model Cards**
   - WER tables, speed comparisons
   - Link: Model READMEs on Hugging Face

3. **ESB Leaderboard**
   - English Speech Benchmark
   - Link: https://hf.co/spaces/hf-audio/open_asr_leaderboard

---

## ğŸ¯ TÃ“M Táº®T EXECUTIVE

### Key Decisions

1. **STT: Upgrade to distil-large-v3**
   - Accuracy: +3-5% WER improvement
   - Speed: Still real-time (5-6x realtime)
   - Cost: +1GB RAM acceptable
   - Timeline: 2 weeks

2. **TTS: Upgrade to Kokoro-82M**
   - Quality: MOS 3.0 â†’ 4.0 (free tier)
   - Size: 82M params (ultra-lightweight)
   - License: Apache 2.0 (commercial-friendly)
   - Timeline: 2 weeks

3. **Voice Cloning: Keep XTTS-v2**
   - Quality: Best voice cloning (proven)
   - Premium: Suitable for premium tier
   - Future: Evaluate MARS6 later
   - Timeline: Deploy in 2 weeks

### Expected Improvements

```yaml
STT:
  Accuracy: 12-15% WER â†’ 9-10% WER (English)
  Quality: +20% reduction in errors
  Cost: +1GB RAM per instance

TTS:
  Quality: 3.0 MOS â†’ 4.0 MOS (free tier)
  User Satisfaction: +33% improvement
  Cost: Neutral (replace gTTS)

End-to-End:
  Accuracy: Better transcriptions
  Quality: Better synthesized speech
  Cost: Acceptable (+1GB RAM)
```

### Next Steps

1. âœ… Get approval for RAM upgrade (if needed)
2. âœ… Week 1-2: Deploy distil-large-v3 (STT)
3. âœ… Week 3-4: Deploy Kokoro-82M (TTS)
4. âœ… Week 5-6: Monitoring & optimization
5. âœ… Week 7-8: Voice cloning evaluation

---

**TÃ i liá»‡u nÃ y cung cáº¥p foundation cho quyáº¿t Ä‘á»‹nh upgrade Audio Models. Táº¥t cáº£ cÃ¡c khuyáº¿n nghá»‹ dá»±a trÃªn research papers, production benchmarks, vÃ  kinh nghiá»‡m thá»±c táº¿ tá»« community.**

**ÄÆ°á»£c nghiÃªn cá»©u vÃ  tá»•ng há»£p bá»Ÿi GitHub Copilot Agent - Dá»±a trÃªn yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng vá» viá»‡c tÃ¬m cÃ¡c distilled models nhanh vÃ  chÃ­nh xÃ¡c cho há»‡ thá»‘ng hiá»‡n táº¡i.**

---

**Document Version:** 1.0
**Last Updated:** January 2025
**Status:** âœ… HOÃ€N THÃ€NH - Sáºµn sÃ ng review vÃ  thá»±c thi
