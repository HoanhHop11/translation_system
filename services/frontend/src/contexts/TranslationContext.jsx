/**
 * TranslationContext - Qu·∫£n l√Ω real-time translation pipeline
 * 
 * Pipeline Flow:
 * 1. Audio Extraction (AudioWorklet) ‚Üí PCM chunks
 * 2. STT Service (/api/v1/transcribe-stream) ‚Üí Text
 * 3. Translation Service (/translate) ‚Üí Translated text
 * 4. TTS Service (/synthesize) ‚Üí Audio
 * 5. Playback (Web Audio API)
 * 
 * Features:
 * - Per-participant translation settings
 * - Language pair management
 * - Caching cho repeated phrases
 * - Toast notifications cho status
 * - Performance monitoring
 */

import React, { createContext, useContext, useState, useCallback, useEffect, useRef } from 'react';
import audioExtractionService from '../services/AudioExtractionService';
import ttsPlaybackService from '../services/TTSPlaybackService';
import localVADService from '../services/LocalVADService';
import { useToast } from './ToastContext';
import { useWebRTC } from './WebRTCContext'; // ‚úÖ Restore import
import { ENV } from '../config/env';

const TranslationContext = createContext();

// Feature flag: use Gateway ASR captions for remote participants (skip remote STT)
const USE_GATEWAY_ASR = true;

// üî• Feature flag: Barge-In - ng·∫Øt TTS khi local user b·∫Øt ƒë·∫ßu n√≥i
const ENABLE_BARGE_IN = true;

// üî• Feature flag: ∆Øu ti√™n s·ª≠ d·ª•ng translation t·ª´ Gateway thay v√¨ g·ªçi API l·∫°i
const USE_SERVER_TRANSLATIONS = true;

// Convert PCM Int16Array to base64 for streaming STT API
const pcm16ToBase64 = (pcmData) => {
  const uint8Array = new Uint8Array(pcmData.buffer);
  let binary = '';

  for (let i = 0; i < uint8Array.byteLength; i++) {
    binary += String.fromCharCode(uint8Array[i]);
  }

  return btoa(binary);
};

export const useTranslation = () => {
  const context = useContext(TranslationContext);
  if (!context) {
    throw new Error('useTranslation must be used within TranslationProvider');
  }
  return context;
};

export const TranslationProvider = ({ children }) => {
  // Translation state
  const [enabled, setEnabled] = useState(false);
  const [ttsEnabled, setTtsEnabled] = useState(true); // Enable TTS playback by default
  const [participantSettings, setParticipantSettings] = useState(new Map());
  const [ttsMode, setTtsMode] = useState('generic'); // 'generic' | 'clone'
  const [ttsReferenceId, setTtsReferenceId] = useState(null); // optional for clone mode
  const [ttsVoice, setTtsVoice] = useState('default'); // 'default' | 'male' | 'female'

  // Global language settings
  const [myLanguage, setMyLanguage] = useState('vi'); // User's language
  const [targetLanguage, setTargetLanguage] = useState('en'); // Translation target
  
  // üî• Auto-TTS: Track if user manually toggled TTS (overrides auto logic)
  const ttsManualOverrideRef = useRef(false);
  
  // Refs for accessing latest state in callbacks/closures
  const myLanguageRef = useRef(myLanguage);
  const targetLanguageRef = useRef(targetLanguage);
  const ttsEnabledRef = useRef(ttsEnabled);

  useEffect(() => { myLanguageRef.current = myLanguage; }, [myLanguage]);
  useEffect(() => { targetLanguageRef.current = targetLanguage; }, [targetLanguage]);
  useEffect(() => { ttsEnabledRef.current = ttsEnabled; }, [ttsEnabled]);

  // Captions state
  const [captions, setCaptions] = useState([]); // Array of { participantId, text, translatedText, timestamp }

  // Performance metrics
  const [metrics, setMetrics] = useState({
    totalTranslations: 0,
    avgLatency: 0,
    errors: 0
  });

  // Cache cho repeated phrases
  const translationCache = useRef(new Map()); // key: `${text}:${srcLang}:${tgtLang}` ‚Üí translation

  // üî• NEW: Sentence buffering for better translation quality
  const transcriptionBuffers = useRef(new Map()); // participantId ‚Üí { chunks: [], lastUpdate: timestamp, timeoutId: null }

  // üî• Utterance-based STT (Offline VI) with simple RMS-VAD segmentation
  const utteranceStates = useRef(new Map()); // participantId ‚Üí { chunks: Int16Array[], totalSamples, isSpeaking, lastSpeech }
  const USE_VI_UTTERANCE_MODE = true; // Toggle ƒë·ªÉ route VI qua utterance endpoint
  const USE_GATEWAY_ASR = true;
  const VAD_CONFIG = {
    rmsThreshold: 0.01,    // Normalized RMS threshold (~-40 dB)
    silenceMs: 800,        // Silence ƒë·ªÉ k·∫øt th√∫c utterance
    minUtteranceMs: 500,   // B·ªè qua utterance qu√° ng·∫Øn
    maxUtteranceMs: 6000   // Flush b·∫Øt bu·ªôc n·∫øu qu√° d√†i
  };

  // TTS-safe mic guard: auto mute local mic during TTS playback to avoid echo-loop
  const ttsMicGuardRef = useRef({
    depth: 0,
    micWasEnabled: null
  });
  // Deduplicate gateway captions to avoid double processing
  const seenGatewayCaptionIds = useRef(new Set());

  // Detect backend translation API shape (VinAI vs NLLB)
  const translationServiceTypeRef = useRef(null); // 'vinai' | 'nllb' | 'unknown'
  
  // Deduplication by content (fix for duplicate events with different IDs)
  const lastProcessedCaptionRef = useRef(new Map()); // participantId -> { text, timestamp }

  // üî• NEW: Track remote audio mute state per participant
  const remoteAudioMuteRef = useRef(new Map()); // participantId -> { wasEnabled, audioTrack }
  
  // üî• Ref ƒë·ªÉ access remoteStreams m·ªõi nh·∫•t trong callbacks (s·∫Ω ƒë∆∞·ª£c set sau useWebRTC)
  const remoteStreamsRef = useRef(new Map());

  const { showToast } = useToast();
  // üî• L·∫•y th√™m serverTranslations v√† remoteStreams ƒë·ªÉ control remote audio
  const { 
    participantId: myParticipantId, 
    localStream,
    remoteStreams, // ƒê·ªÉ access remote audio tracks
    serverTranslations, // Pre-translated text t·ª´ Gateway
    participants // ƒê·ªÉ bi·∫øt language c·ªßa remote participants
  } = useWebRTC();
  
  // üî• Keep ref updated v·ªõi remoteStreams m·ªõi nh·∫•t - LU√îN sync
  remoteStreamsRef.current = remoteStreams;

  // Service URLs (use centralized ENV config)
  const STT_SERVICE_URL = ENV.STT_SERVICE_URL;
  const TRANSLATION_SERVICE_URL = ENV.TRANSLATION_SERVICE_URL;
  const TTS_SERVICE_URL = ENV.TTS_SERVICE_URL;

  /**
   * Enable/disable translation
   */
  const toggleTranslation = useCallback((value) => {
    const newEnabled = value !== undefined ? value : !enabled;

    // Avoid redundant re-renders/log spam
    if (newEnabled === enabled) {
      return;
    }

    setEnabled(newEnabled);

    if (newEnabled) {
      showToast('Translation enabled', 'success');
      console.log('üåê Translation enabled', { myLanguage, targetLanguage });
    } else {
      showToast('Translation disabled', 'info');
      console.log('üåê Translation disabled');

      // Stop all extractions and playback
      audioExtractionService.stopAll();
      ttsPlaybackService.stopAll();
    }
  }, [enabled, myLanguage, targetLanguage, showToast]);

  /**
   * Toggle TTS playback (internal implementation)
   * üî• Mute/unmute ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi useEffect [remoteStreams, ttsEnabled] ƒë·ªÉ ƒë·∫£m b·∫£o sync
   * @param isManual - true n·∫øu user toggle th·ªß c√¥ng (override auto logic)
   */
  const toggleTTSInternal = useCallback((value, isManual = false) => {
    const newTtsEnabled = value !== undefined ? value : !ttsEnabled;

    // Avoid double toggles from repeated clicks/rerenders
    if (newTtsEnabled === ttsEnabled) {
      return;
    }

    // üî• Mark manual override if user toggled manually
    if (isManual) {
      ttsManualOverrideRef.current = true;
      console.log('üéöÔ∏è [Auto-TTS] Manual override set - auto-TTS disabled');
    }

    if (newTtsEnabled) {
      showToast('Live Translation enabled - Remote audio muted', 'success');
      console.log('üîä TTS playback enabled', isManual ? '(manual)' : '(auto)');
    } else {
      showToast('Live Translation disabled - Original audio restored', 'info');
      console.log('üîá TTS playback disabled', isManual ? '(manual)' : '(auto)');
      
      // Stop current playback
      ttsPlaybackService.stopAll();
      
      // Clear mute state tracking
      remoteAudioMuteRef.current.clear();
    }

    // üî• Set state - useEffect [remoteStreams, ttsEnabled] s·∫Ω handle mute/unmute
    setTtsEnabled(newTtsEnabled);
  }, [ttsEnabled, showToast]);

  /**
   * Toggle TTS playback (public API - marks as manual override)
   */
  const toggleTTS = useCallback((value) => {
    toggleTTSInternal(value, true);
  }, [toggleTTSInternal]);

  /**
   * Setup translation cho m·ªôt participant
   */
  const setupParticipantTranslation = useCallback(async (participantId, audioTrack) => {
    if (!enabled) {
      console.log(`‚è∏Ô∏è Translation disabled, skipping setup for ${participantId}`);
      return;
    }

    // Khi d√πng Gateway ASR, b·ªè qua setup STT cho t·∫•t c·∫£ (remote + local) ƒë·ªÉ tr√°nh double STT
    if (USE_GATEWAY_ASR) {
      console.log(`‚è≠Ô∏è Skipping STT setup for ${participantId} (Gateway ASR mode)`);
      return;
    }

    try {
      console.log(`üîß Setting up translation for ${participantId}`);

      // Setup audio extraction v·ªõi callback
      await audioExtractionService.setupExtraction(
        participantId,
        audioTrack,
        (audioChunk) => handleAudioChunk(participantId, audioChunk)
      );

      // Initialize participant settings
      setParticipantSettings(prev => {
        const newMap = new Map(prev);
        newMap.set(participantId, {
          sourceLanguage: 'auto', // Auto-detect
          targetLanguage: targetLanguage,
          enabled: true,
          transcriptionBuffer: []
        });
        return newMap;
      });

      console.log(`‚úÖ Translation setup complete for ${participantId}`);

    } catch (error) {
      console.error(`‚ùå Error setting up translation for ${participantId}:`, error);
      showToast(`Translation setup failed: ${error.message}`, 'error');
    }
  }, [enabled, targetLanguage, showToast]);

  /**
   * Enhanced capitalization normalization v·ªõi Vietnamese proper noun detection
   */
  const normalizeCapitalization = (text) => {
    if (!text) return text;

    // Gi·ªØ nguy√™n hoa/th∆∞·ªùng g·ªëc, ch·ªâ ƒë·∫£m b·∫£o ch·ªØ c√°i ƒë·∫ßu c√¢u vi·∫øt hoa n·∫øu ƒëang ·ªü d·∫°ng to√†n th∆∞·ªùng
    const trimmed = text.trim();
    if (trimmed.length === 0) return trimmed;

    // N·∫øu text ƒëang to√†n ch·ªØ hoa, chuy·ªÉn v·ªÅ sentence case ƒë·ªÉ tr√°nh hi·ªÉn th·ªã to√†n caps
    const isAllCaps = trimmed === trimmed.toUpperCase();
    if (isAllCaps) {
      const lowered = trimmed.toLowerCase();
      return lowered.charAt(0).toUpperCase() + lowered.slice(1);
    }

    // N·∫øu kh√¥ng, vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu c√¢u
    return trimmed.charAt(0).toUpperCase() + trimmed.slice(1);
  };

  /**
   * Stop translation cho participant
   */
  const stopParticipantTranslation = useCallback(async (participantId) => {
    console.log(`üõë Stopping translation for ${participantId}`);

    await audioExtractionService.stopExtraction(participantId);
    ttsPlaybackService.stopPlayback(participantId);

    setParticipantSettings(prev => {
      const newMap = new Map(prev);
      newMap.delete(participantId);
      return newMap;
    });
  }, []);

  /**
   * Handle audio chunk t·ª´ AudioWorklet v·ªõi sentence buffering
   */
  const handleAudioChunk = useCallback(async (participantId, audioChunk) => {
    if (USE_GATEWAY_ASR) {
      // Gateway ch·ªãu tr√°ch nhi·ªám STT, b·ªè qua client STT
      return;
    }
    // Access current state via refs to avoid closure staleness
    const currentMyLanguage = myLanguageRef.current;
    const currentTargetLanguage = targetLanguageRef.current;
    const currentTtsEnabled = ttsEnabledRef.current;

    const useUtteranceMode = USE_VI_UTTERANCE_MODE && currentMyLanguage === 'vi';
    // Utterance mode (Offline VI) v·ªõi VAD segmentation
    if (useUtteranceMode) {
      await handleAudioChunkUtterance(participantId, audioChunk);
      return;
    }

    const startTime = Date.now();

    try {
      const { pcmData, sampleRate, duration, chunkIndex } = audioChunk;

      // Step 1: STT - Transcribe audio (streaming endpoint expects base64 PCM)
      const transcription = await transcribeAudio({
        participantId,
        pcmData,
        sampleRate,
        chunkIndex,
        language: currentMyLanguage
      });

      // Skip empty/interim chunks
      if (!transcription || !transcription.text || transcription.text.trim() === '') {
        console.log(`‚è≠Ô∏è Empty transcription for ${participantId}, skipping`);
        return;
      }

      if (transcription.is_final === false) {
        console.log(`‚è≠Ô∏è Interim transcription for ${participantId}, waiting for final`);
        return;
      }

      // üî• BUFFERING LOGIC: Accumulate chunks into sentences
      const buffer = transcriptionBuffers.current.get(participantId) || {
        chunks: [],
        lastUpdate: Date.now(),
        timeoutId: null
      };

      // Clear existing timeout
      if (buffer.timeoutId) {
        clearTimeout(buffer.timeoutId);
      }

      // Add chunk to buffer
      buffer.chunks.push(transcription.text.trim());
      buffer.lastUpdate = Date.now();

      // Set flush timeout (3 seconds of silence)
      const flushBuffer = async () => {
        const currentBuffer = transcriptionBuffers.current.get(participantId);
        if (!currentBuffer || currentBuffer.chunks.length === 0) return;

        // Combine all chunks into complete sentence
        const fullSentence = currentBuffer.chunks.join(' ').trim();
        transcriptionBuffers.current.delete(participantId);

        // Skip very short sentences
        if (fullSentence.length < 3) {
          console.log(`‚è≠Ô∏è Sentence too short, skipping: "${fullSentence}"`);
          return;
        }

        console.log(`üìù Complete sentence for ${participantId} (${currentBuffer.chunks.length} chunks):`, fullSentence);

        // Normalize capitalization
        const normalizedText = normalizeCapitalization(fullSentence);

        // Step 2: Translation - Translate complete sentence
        const sourceLanguage = transcription.language || 'auto';
        const translated = await translateText(
          normalizedText,
          sourceLanguage,
          currentTargetLanguage
        );

        console.log(`üåê Translation for ${participantId}:`, translated);

        // Step 3: TTS - Only play for REMOTE participants (not self)
        if (currentTtsEnabled && participantId !== myParticipantId) {
          const audioBase64 = await synthesizeSpeech(translated, currentTargetLanguage);

          await ttsPlaybackService.playTranslatedAudio(participantId, audioBase64, {
            immediate: true,
            voice: ttsVoice,
            lang: currentTargetLanguage,
            onStart: () => handleTTSAudioStart(),
            onEnd: () => handleTTSAudioEnd()
          });
        } else if (participantId === myParticipantId) {
          console.log(`üîá Skipping TTS for own audio (self=${myParticipantId})`);
        } else {
          console.log(`üîá TTS disabled, caption only mode`);
        }

        // Update captions with complete sentence
        const caption = {
          id: `${participantId}-${Date.now()}`,
          participantId,
          text: normalizedText,
          translatedText: translated,
          timestamp: Date.now(),
          language: sourceLanguage
        };

        setCaptions(prev => [...prev.slice(-9), caption]); // Keep last 10

        // Update metrics
        const latency = Date.now() - startTime;
        setMetrics(prev => ({
          totalTranslations: prev.totalTranslations + 1,
          avgLatency: (prev.avgLatency * prev.totalTranslations + latency) / (prev.totalTranslations + 1),
          errors: prev.errors
        }));

        console.log(`‚úÖ Translation pipeline complete for ${participantId}`, {
          latency: `${latency}ms`,
          sentence: normalizedText.substring(0, 50) + '...'
        });
      };

      buffer.timeoutId = setTimeout(flushBuffer, 3000); // 3 second buffer timeout
      transcriptionBuffers.current.set(participantId, buffer);

      console.log(`‚è∏Ô∏è Buffering chunk ${buffer.chunks.length} for ${participantId}: "${transcription.text}"`);

    } catch (error) {
      console.error(`‚ùå Translation pipeline error for ${participantId}:`, error);

      setMetrics(prev => ({
        ...prev,
        errors: prev.errors + 1
      }));
    }
  }, [myParticipantId, showToast]); // Removed state deps, using refs

  /**
   * Simple RMS-based VAD + utterance segmentation (Offline VI)
   * - Accumulate until speech ends (silenceMs) or maxUtteranceMs reached
   * - Flush as a single utterance to offline VI endpoint
   */
  const handleAudioChunkUtterance = useCallback(async (participantId, audioChunk) => {
    const { pcmData, sampleRate } = audioChunk;

    // Compute normalized RMS (0..1)
    const rms = computeRmsInt16(pcmData);
    const now = Date.now();

    const state = utteranceStates.current.get(participantId) || {
      chunks: [],
      totalSamples: 0,
      isSpeaking: false,
      lastSpeech: 0
    };

    const durationMs = state.totalSamples / sampleRate * 1000;

    if (rms > VAD_CONFIG.rmsThreshold) {
      state.isSpeaking = true;
      state.lastSpeech = now;
      state.chunks.push(pcmData);
      state.totalSamples += pcmData.length;

      if (durationMs >= VAD_CONFIG.maxUtteranceMs) {
        await flushUtterance(participantId, state, sampleRate);
        // flushUtterance s·∫Ω t·ª± reset state trong utteranceStates
        return;
      }
    } else {
      if (state.isSpeaking && (now - state.lastSpeech) >= VAD_CONFIG.silenceMs) {
        await flushUtterance(participantId, state, sampleRate);
        // flushUtterance s·∫Ω t·ª± reset state trong utteranceStates
        return;
      }
    }

    utteranceStates.current.set(participantId, state);
  }, [myParticipantId]); // Refs handled inside flushUtterance

  /**
   * Flush current utterance (if any) and run STT -> Translate -> Caption/TTS
   */
  const flushUtterance = useCallback(async (participantId, state, sampleRate) => {
    const currentTargetLanguage = targetLanguageRef.current;
    const currentTtsEnabled = ttsEnabledRef.current;

    if (!state || state.chunks.length === 0) {
      resetUtteranceState(participantId);
      return;
    }

    const totalMs = state.totalSamples / sampleRate * 1000;
    if (totalMs < VAD_CONFIG.minUtteranceMs) {
      resetUtteranceState(participantId);
      return;
    }

    const startTime = Date.now();
    const merged = mergeInt16Chunks(state.chunks, state.totalSamples);
    resetUtteranceState(participantId);

    try {
      const transcription = await transcribeUtterance({
        participantId,
        pcmData: merged,
        sampleRate
      });

      if (!transcription || !transcription.text || transcription.text.trim() === '') {
        console.log(`‚è≠Ô∏è Empty utterance transcription for ${participantId}, skipping`);
        return;
      }

      const normalizedText = normalizeCapitalization(transcription.text.trim());
      const sourceLanguage = transcription.language || 'vi';
      const translated = await translateText(
        normalizedText,
        sourceLanguage,
        currentTargetLanguage
      );

      if (currentTtsEnabled && participantId !== myParticipantId) {
        const audioBase64 = await synthesizeSpeech(translated, currentTargetLanguage);
        await ttsPlaybackService.playTranslatedAudio(participantId, audioBase64, {
          immediate: true,
          voice: ttsVoice,
          lang: currentTargetLanguage,
          onStart: () => handleTTSAudioStart(),
          onEnd: () => handleTTSAudioEnd()
        });
      }

      const caption = {
        id: `${participantId}-${Date.now()}`,
        participantId,
        text: normalizedText,
        translatedText: translated,
        timestamp: Date.now(),
        language: sourceLanguage
      };
      setCaptions(prev => [...prev.slice(-9), caption]);

      const latency = Date.now() - startTime;
      setMetrics(prev => ({
        totalTranslations: prev.totalTranslations + 1,
        avgLatency: (prev.avgLatency * prev.totalTranslations + latency) / (prev.totalTranslations + 1),
        errors: prev.errors
      }));

      console.log(`‚úÖ Utterance pipeline complete for ${participantId}`, {
        latency: `${latency}ms`,
        sentence: normalizedText.substring(0, 50) + '...'
      });
    } catch (error) {
      console.error(`‚ùå Utterance pipeline error for ${participantId}:`, error);
      setMetrics(prev => ({ ...prev, errors: prev.errors + 1 }));
    }
  }, [myParticipantId]);

  const resetUtteranceState = (participantId) => {
    utteranceStates.current.set(participantId, {
      chunks: [],
      totalSamples: 0,
      isSpeaking: false,
      lastSpeech: 0
    });
  };

  const computeRmsInt16 = (pcmInt16) => {
    let sumSq = 0;
    for (let i = 0; i < pcmInt16.length; i++) {
      const v = pcmInt16[i] / 32768;
      sumSq += v * v;
    }
    return Math.sqrt(sumSq / pcmInt16.length);
  };

  const mergeInt16Chunks = (chunks, totalSamples) => {
    const merged = new Int16Array(totalSamples);
    let offset = 0;
    for (const chunk of chunks) {
      merged.set(chunk, offset);
      offset += chunk.length;
    }
    return merged;
  };

  /**
   * TTS-safe helpers: DISABLED - kh√¥ng mute local mic n·ªØa
   * L√Ω do: Barge-In ƒë√£ x·ª≠ l√Ω vi·ªác ng·∫Øt TTS khi user n√≥i
   * Mute mic g√¢y ra v·∫•n ƒë·ªÅ: User kh√¥ng th·ªÉ n√≥i khi TTS ƒëang ph√°t
   */
  const handleTTSAudioStart = () => {
    // üî• DISABLED: Kh√¥ng mute mic n·ªØa - Barge-In s·∫Ω x·ª≠ l√Ω
    // Vi·ªác mute mic khi·∫øn user kh√¥ng th·ªÉ n√≥i khi TTS ƒëang ph√°t
    // v√† Gateway kh√¥ng nh·∫≠n ƒë∆∞·ª£c audio ‚Üí kh√¥ng c√≥ caption
    console.log('üîä TTS playback started (mic NOT muted - Barge-In enabled)');
  };

  const handleTTSAudioEnd = () => {
    // üî• DISABLED: Kh√¥ng c·∫ßn restore mic v√¨ kh√¥ng mute
    console.log('üîä TTS playback ended');
  };

  /**
   * üî• NEW: Mute remote audio track khi TTS ƒëang ph√°t
   * Logic ki·ªÉm tra ng√¥n ng·ªØ ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü ingestGatewayCaption
   */
  const muteRemoteAudio = useCallback((speakerId) => {
    // üî• D√πng ref ƒë·ªÉ c√≥ gi√° tr·ªã m·ªõi nh·∫•t
    const remoteStream = remoteStreamsRef.current?.get?.(speakerId);
    if (!remoteStream) {
      console.log(`‚ö†Ô∏è No remote stream found for ${speakerId}`);
      return false;
    }

    const audioTrack = remoteStream.getAudioTracks()?.[0];
    if (!audioTrack) {
      console.log(`‚ö†Ô∏è No audio track found for ${speakerId}`);
      return false;
    }

    // L∆∞u tr·∫°ng th√°i v√† mute
    const muteState = remoteAudioMuteRef.current.get(speakerId) || { depth: 0, wasEnabled: null };
    
    if (muteState.depth === 0) {
      muteState.wasEnabled = audioTrack.enabled;
      if (audioTrack.enabled) {
        audioTrack.enabled = false;
        console.log(`üîá Muting remote audio for ${speakerId}`);
      }
    }
    muteState.depth += 1;
    remoteAudioMuteRef.current.set(speakerId, muteState);
    
    return true; // ƒê√£ mute
  }, []); // Kh√¥ng c·∫ßn dependency v√¨ d√πng ref

  /**
   * üî• NEW: Restore remote audio track sau khi TTS ph√°t xong
   */
  const unmuteRemoteAudio = useCallback((speakerId) => {
    const muteState = remoteAudioMuteRef.current.get(speakerId);
    if (!muteState) return;

    if (muteState.depth > 0) {
      muteState.depth -= 1;
    }

    if (muteState.depth === 0 && muteState.wasEnabled !== null) {
      // üî• D√πng ref ƒë·ªÉ c√≥ gi√° tr·ªã m·ªõi nh·∫•t
      const remoteStream = remoteStreamsRef.current?.get?.(speakerId);
      const audioTrack = remoteStream?.getAudioTracks()?.[0];
      
      if (audioTrack && muteState.wasEnabled) {
        audioTrack.enabled = true;
        console.log(`üîä Restoring remote audio for ${speakerId}`);
      }
      
      remoteAudioMuteRef.current.delete(speakerId);
    }
  }, []); // Kh√¥ng c·∫ßn dependency v√¨ d√πng ref

  /**
   * Transcribe audio v·ªõi STT service
   */
  const transcribeAudio = async ({ participantId, pcmData, sampleRate, chunkIndex, language }) => {
    const audioBase64 = pcm16ToBase64(pcmData);

    const response = await fetch(`${STT_SERVICE_URL}/api/v1/transcribe-stream`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        participant_id: participantId,
        audio_data: audioBase64,
        sample_rate: sampleRate,
        channels: 1,
        format: 'pcm16',
        language: language || 'auto',
        chunk_id: chunkIndex
      })
    });

    if (!response.ok) {
      console.error('‚ùå STT request failed', response.status, response.statusText);
      throw new Error(`STT failed: ${response.status}`);
    }

    const result = await response.json();
    return result;
  };

  /**
   * Transcribe utterance (Offline VI endpoint)
   */
  const transcribeUtterance = async ({ participantId, pcmData, sampleRate }) => {
    const audioBase64 = pcm16ToBase64(pcmData);

    const response = await fetch(`${STT_SERVICE_URL}/api/v1/transcribe-vi-utterance`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        participant_id: participantId,
        audio_data: audioBase64,
        sample_rate: sampleRate,
        channels: 1,
        format: 'pcm16',
        language: 'vi'
      })
    });

    if (!response.ok) {
      console.error('‚ùå Utterance STT request failed', response.status, response.statusText);
      throw new Error(`Utterance STT failed: ${response.status}`);
    }

    const result = await response.json();
    return result;
  };

  /**
   * Translate text v·ªõi Translation service
   */
  const detectTranslationServiceType = async () => {
    if (translationServiceTypeRef.current) {
      return translationServiceTypeRef.current;
    }

    // Use configured type if set (skip detection)
    const configuredType = (ENV.TRANSLATION_SERVICE_TYPE || '').toLowerCase();
    if (configuredType && configuredType !== 'auto') {
      console.log(`üîß Using configured translation service type: ${configuredType}`);
      translationServiceTypeRef.current = configuredType;
      return configuredType;
    }

    try {
      const res = await fetch(`${TRANSLATION_SERVICE_URL}/`);
      if (res.ok) {
        const data = await res.json();
        const serviceName = (data.service || '').toLowerCase();

        if (serviceName.includes('vinai')) {
          translationServiceTypeRef.current = 'vinai';
        } else {
          translationServiceTypeRef.current = 'nllb';
        }
      } else {
        translationServiceTypeRef.current = 'unknown';
      }
    } catch (e) {
      console.warn('‚ö†Ô∏è Could not detect translation service type, defaulting to NLLB-style API', e);
      translationServiceTypeRef.current = 'unknown';
    }

    return translationServiceTypeRef.current;
  };

  const translateText = async (text, srcLang, tgtLang) => {
    // Normalize language codes (handle cases like "vi-VN" or "auto")
    let normalizedSrc = (srcLang || myLanguage || 'vi').split('-')[0];
    let normalizedTgt = (tgtLang || targetLanguage || (normalizedSrc === 'vi' ? 'en' : 'vi')).split('-')[0];

    // Check cache
    const cacheKey = `${text}:${normalizedSrc}:${normalizedTgt}`;
    if (translationCache.current.has(cacheKey)) {
      console.log('‚úÖ Translation cache hit');
      return translationCache.current.get(cacheKey);
    }

    const serviceType = await detectTranslationServiceType();

    let response;

    if (serviceType === 'vinai') {
      const direction =
        normalizedSrc === 'vi' && normalizedTgt === 'en'
          ? 'vi2en'
          : normalizedSrc === 'en' && normalizedTgt === 'vi'
          ? 'en2vi'
          : normalizedSrc === 'vi'
          ? 'vi2en'
          : 'en2vi';

      response = await fetch(`${TRANSLATION_SERVICE_URL}/translate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          text,
          direction
        })
      });
    } else {
      // Default: NLLB-style generic translation service
      response = await fetch(`${TRANSLATION_SERVICE_URL}/translate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          text,
          src_lang: normalizedSrc,
          tgt_lang: normalizedTgt,
          use_cache: true
        })
      });
    }

    if (!response.ok) {
      throw new Error(`Translation failed: ${response.status}`);
    }

    const result = await response.json();
    const translated =
      result.translated_text || // NLLB service
      result.text ||            // VinAI service
      '';                       // Fallback (should not happen)

    // Cache result
    translationCache.current.set(cacheKey, translated);

    return translated;
  };

  /**
   * Synthesize speech v·ªõi TTS service
   */
  const synthesizeSpeech = async (text, language) => {
    const normalizedLang = (language || targetLanguage || 'en').split('-')[0];

    const payload = {
      text,
      // Backward-compat fields
      language: normalizedLang, // legacy field
      // New fields for Piper/OpenVoice
      lang: normalizedLang,
      mode: ttsMode || 'generic',
    };

    if (ttsReferenceId) {
      payload.reference_id = ttsReferenceId;
    }

    const response = await fetch(`${TTS_SERVICE_URL}/synthesize`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      throw new Error(`TTS failed: ${response.status}`);
    }

    const result = await response.json();
    return result.audio_base64 || result.audio || '';
  };

  /**
   * Clear captions
   */
  const clearCaptions = useCallback(() => {
    setCaptions([]);
  }, []);

  /**
   * üî• Helper: Check xem Gateway ƒë√£ g·ª≠i translation cho text n√†y ch∆∞a
   * N·∫øu c√≥ th√¨ d√πng lu√¥n, kh√¥ng c·∫ßn g·ªçi Translation API
   */
  const getServerTranslation = useCallback((participantId, text, tgtLang) => {
    if (!USE_SERVER_TRANSLATIONS || !serverTranslations) return null;
    
    const key = `${participantId}-${text?.trim()}-${tgtLang}`;
    const cached = serverTranslations.get(key);
    
    if (cached && cached.translatedText) {
      console.log('‚úÖ Using server-side translation (no duplicate API call):', {
        key: key.substring(0, 50) + '...',
        translatedText: cached.translatedText.substring(0, 30) + '...'
      });
      return cached.translatedText;
    }
    
    return null;
  }, [serverTranslations]);

  /**
   * Ingest caption t·ª´ Gateway (ASR server) v√† ch·∫°y MT/TTS per-viewer
   * üî• OPTIMIZED: Check serverTranslations tr∆∞·ªõc khi g·ªçi Translation API
   */
  const ingestGatewayCaption = useCallback(async (caption) => {
    try {
      if (!enabled) return;
      if (!caption || !caption.text || caption.text.trim() === '') return;
      
      // 1. Check duplicate by ID (legacy check)
      if (caption.id && seenGatewayCaptionIds.current.has(caption.id)) {
        return;
      }
      
      // 2. Check duplicate by Content & Time (fix for Gateway sending same text with new IDs)
      const speakerKey = caption.speakerId || caption.participantId || 'unknown';
      const lastCap = lastProcessedCaptionRef.current.get(speakerKey);
      const now = Date.now();
      const capTime = caption.timestamp || now;
      
      if (lastCap) {
        const timeDiff = Math.abs(capTime - lastCap.timestamp);
        // N·∫øu n·ªôi dung gi·ªëng h·ªát v√† th·ªùi gian c√°ch nhau < 2s -> coi l√† duplicate
        if (lastCap.text === caption.text.trim() && timeDiff < 2000) {
          console.log(`‚ôªÔ∏è Duplicate caption content detected for ${caption.speakerId}: "${caption.text}" (diff: ${timeDiff}ms)`);
          // V·∫´n add ID v√†o set ƒë·ªÉ ch·∫∑n c√°c l·∫ßn sau n·∫øu d√πng ID c≈©
          if (caption.id) seenGatewayCaptionIds.current.add(caption.id);
          return;
        }
      }

      if (caption.id) {
        seenGatewayCaptionIds.current.add(caption.id);
      }
      
      // Update last processed
      lastProcessedCaptionRef.current.set(speakerKey, {
        text: caption.text.trim(),
        timestamp: capTime
      });

      const normalizedText = normalizeCapitalization(caption.text.trim());
      const sourceLanguage = caption.language || 'auto';

      // üî• LOGIC FIX: 
      // - Remote speaker n√≥i ng√¥n ng·ªØ X (sourceLanguage t·ª´ caption)
      // - User mu·ªën nghe b·∫±ng ng√¥n ng·ªØ c·ªßa m√¨nh (myLanguage)
      // - D·ªãch: sourceLanguage ‚Üí myLanguage
      // - TTS ph√°t b·∫±ng: myLanguage
      const userLanguage = myLanguageRef.current || myLanguage || 'vi';
      
      // Check xem Gateway ƒë√£ translate ch∆∞a (d√πng myLanguage l√†m target)
      let translated = getServerTranslation(speakerKey, caption.text.trim(), userLanguage);
      
      if (!translated) {
        // Fallback: G·ªçi Translation API - d·ªãch t·ª´ source ‚Üí user's language
        console.log(`‚ö° No server translation found, translating ${sourceLanguage} ‚Üí ${userLanguage}...`);
        translated = await translateText(normalizedText, sourceLanguage, userLanguage);
      }

      // TTS n·∫øu b·∫≠t v√† kh√¥ng ph·∫£i self
      // üîä Logic ƒë∆°n gi·∫£n: Khi TTS b·∫≠t ‚Üí remote audio ƒë√£ mute ‚Üí ph√°t TTS b·∫±ng ng√¥n ng·ªØ c·ªßa user
      if (ttsEnabled && caption.speakerId && caption.speakerId !== myParticipantId) {
        console.log(`üé§ TTS enabled, playing translated audio in ${userLanguage} for ${caption.speakerId}`);
        
        // TTS ph√°t b·∫±ng ng√¥n ng·ªØ c·ªßa USER (myLanguage), kh√¥ng ph·∫£i targetLanguage
        const audioBase64 = await synthesizeSpeech(translated, userLanguage);
        await ttsPlaybackService.playTranslatedAudio(caption.speakerId, audioBase64, {
          immediate: true,
          voice: ttsVoice,
          lang: userLanguage,
          onStart: () => handleTTSAudioStart(),
          onEnd: () => handleTTSAudioEnd()
        });
      }

      const nextCaption = {
        id: caption.id || `${caption.speakerId || 'unknown'}-${caption.timestamp || Date.now()}`,
        participantId: caption.speakerId,
        text: normalizedText,
        translatedText: translated,
        timestamp: caption.timestamp || Date.now(),
        language: sourceLanguage
      };

      setCaptions(prev => [...prev.slice(-9), nextCaption]);
    } catch (err) {
      console.error('‚ùå ingestGatewayCaption error:', err);
    }
  }, [enabled, myLanguage, ttsEnabled, myParticipantId, getServerTranslation]);

  /**
   * Get translation stats
   */
  const getStats = useCallback(() => {
    return {
      enabled,
      participantCount: participantSettings.size,
      captionCount: captions.length,
      cacheSize: translationCache.current.size,
      metrics,
      audioExtraction: audioExtractionService.getStats(),
      ttsPlayback: ttsPlaybackService.getStats()
    };
  }, [enabled, participantSettings, captions, metrics]);

  // üî• Barge-In: Start LocalVAD khi c√≥ localStream v√† translation enabled
  // Khi local user n√≥i, ng·∫Øt TTS ƒëang ph√°t (n·∫øu c√≥)
  useEffect(() => {
    if (!ENABLE_BARGE_IN || !enabled || !ttsEnabled || !localStream) {
      localVADService.stop();
      return;
    }

    // Start LocalVAD v·ªõi callbacks
    localVADService.start(localStream, {
      onSpeechStart: () => {
        // Barge-In: Ng·∫Øt TTS ngay l·∫≠p t·ª©c khi local user n√≥i
        const wasPlaying = ttsPlaybackService.interruptForBargeIn(true);
        
        if (wasPlaying) {
          console.log('üõë [Barge-In] TTS interrupted - local user is speaking');
          
          // üî• D√πng ref ƒë·ªÉ c√≥ gi√° tr·ªã m·ªõi nh·∫•t - Unmute remote audio cho t·∫•t c·∫£ participants
          const currentRemoteStreams = remoteStreamsRef.current;
          if (currentRemoteStreams) {
            for (const [speakerId] of currentRemoteStreams) {
              unmuteRemoteAudio(speakerId);
            }
          }
        }
      },
      onSpeechEnd: () => {
        console.log('ü§ê [Barge-In] Local user stopped speaking');
        // Kh√¥ng c·∫ßn l√†m g√¨ - pipeline ti·∫øp t·ª•c b√¨nh th∆∞·ªùng
      }
    });

    console.log('üé§ [Barge-In] LocalVAD started for local speech detection');

    return () => {
      localVADService.stop();
      console.log('üé§ [Barge-In] LocalVAD stopped');
    };
  }, [enabled, ttsEnabled, localStream, unmuteRemoteAudio]);

  // üî• Auto-TTS: T·ª± ƒë·ªông b·∫≠t/t·∫Øt TTS d·ª±a tr√™n language pair
  // - C√πng ng√¥n ng·ªØ: t·∫Øt TTS (kh√¥ng c·∫ßn d·ªãch)
  // - Kh√°c ng√¥n ng·ªØ: b·∫≠t TTS
  // - Ch·ªâ ho·∫°t ƒë·ªông khi user ch∆∞a toggle manual
  useEffect(() => {
    // B·ªè qua n·∫øu user ƒë√£ toggle manual ho·∫∑c kh√¥ng c√≥ participants
    if (ttsManualOverrideRef.current || !participants || participants.size === 0) {
      return;
    }

    // Ki·ªÉm tra ng√¥n ng·ªØ c·ªßa remote participants
    let hasRemoteWithDifferentLanguage = false;
    
    for (const [remotePId, pData] of participants) {
      // Skip local participant
      if (remotePId === myParticipantId) continue;
      
      const remoteLanguage = pData.sourceLanguage || pData.targetLanguage;
      
      if (remoteLanguage && remoteLanguage !== myLanguage) {
        hasRemoteWithDifferentLanguage = true;
        break;
      }
    }

    // Auto-toggle TTS based on language pair (kh√¥ng g·ªçi toggleTTS ƒë·ªÉ tr√°nh mark manual override)
    if (hasRemoteWithDifferentLanguage && !ttsEnabled) {
      console.log('üîÑ [Auto-TTS] Kh√°c ng√¥n ng·ªØ detected ‚Üí B·∫≠t TTS');
      toggleTTSInternal(true, false);
    } else if (!hasRemoteWithDifferentLanguage && ttsEnabled) {
      console.log('üîÑ [Auto-TTS] C√πng ng√¥n ng·ªØ detected ‚Üí T·∫Øt TTS');
      toggleTTSInternal(false, false);
    }
  }, [myLanguage, myParticipantId, participants, ttsEnabled, toggleTTSInternal]);

  // Reset manual override khi r·ªùi ph√≤ng ho·∫∑c participants thay ƒë·ªïi ƒë√°ng k·ªÉ
  useEffect(() => {
    if (!participants || participants.size === 0) {
      // Reset manual override khi kh√¥ng c√≤n ai trong room
      ttsManualOverrideRef.current = false;
      console.log('üîÑ [Auto-TTS] Reset manual override (empty room)');
    }
  }, [participants]);

  // üî• Auto-sync remote audio mute state v·ªõi TTS enabled state
  // ƒê·∫£m b·∫£o tr·∫°ng th√°i mute LU√îN ƒë√∫ng theo ttsEnabled v√† remoteStreams hi·ªán t·∫°i
  // - TTS enabled (ttsEnabled=true) => remote audio muted (track.enabled=false)
  // - TTS disabled (ttsEnabled=false) => remote audio unmuted (track.enabled=true)
  useEffect(() => {
    if (!remoteStreams || remoteStreams.size === 0) {
      return;
    }

    // desiredEnabled = !ttsEnabled
    // ttsEnabled=true => track should be disabled (muted)
    // ttsEnabled=false => track should be enabled (unmuted)
    const desiredTrackEnabled = !ttsEnabled;

    for (const [participantId, stream] of remoteStreams.entries()) {
      const audioTracks = stream?.getAudioTracks?.() || [];
      for (const track of audioTracks) {
        if (track.enabled !== desiredTrackEnabled) {
          track.enabled = desiredTrackEnabled;
          console.log(
            `üîä [Auto-Sync-Mute] ${participantId} track.enabled=${track.enabled} (ttsEnabled=${ttsEnabled})`
          );
        }
      }
    }
  }, [remoteStreams, ttsEnabled]);

  // Cleanup khi unmount
  useEffect(() => {
    return () => {
      audioExtractionService.stopAll();
      ttsPlaybackService.stopAll();
      localVADService.stop();
    };
  }, []);

  const value = {
    // State
    enabled,
    myLanguage,
    targetLanguage,
    captions,
    metrics,
    participantSettings,
    ttsEnabled,
    ttsMode,
    ttsReferenceId,
    ttsVoice,

    // Actions
    toggleTranslation,
    toggleTTS,
    setMyLanguage,
    setTargetLanguage,
    setTtsMode,
    setTtsReferenceId,
    setTtsVoice,
    setupParticipantTranslation,
    stopParticipantTranslation,
    clearCaptions,
    ingestGatewayCaption,
    getStats,

    // Services (expose for advanced usage)
    audioExtractionService,
    ttsPlaybackService,
    localVADService,
    
    // Barge-In status
    bargeInEnabled: ENABLE_BARGE_IN
  };

  return (
    <TranslationContext.Provider value={value}>
      {children}
    </TranslationContext.Provider>
  );
};
